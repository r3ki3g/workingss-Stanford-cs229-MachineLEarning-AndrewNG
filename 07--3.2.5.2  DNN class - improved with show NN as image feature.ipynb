{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e285cc3c",
   "metadata": {},
   "source": [
    "## What's new in this implementation\n",
    "<font color=\"red\">\n",
    "  Now the user can get a image of the whole NN architecture\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4971c84a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T05:28:40.377335Z",
     "start_time": "2023-12-06T05:28:38.944310Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import HTML,Latex\n",
    "import os \n",
    "from joblib import Parallel,delayed\n",
    "import json\n",
    "import cv2 as cv\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "585e56ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T08:16:54.854696Z",
     "start_time": "2023-12-06T08:16:54.785145Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "PARAM_FOLDER_NAME = 'parameters_as_json/'\n",
    "class DNN:\n",
    "    def __init__(self,layers):\n",
    "        # layers :  list of dicts containing later information.\n",
    "        assert type(layers) in  (list,tuple)\n",
    "        assert len(layers) >= 2, \"At least an input and an output layer should be there\"\n",
    "        \n",
    "        # validate each layer\n",
    "        for i,layer in enumerate(layers):\n",
    "            if i==0:\n",
    "                assert layer[\"type\"] == \"input\", \"first layer should be input\"\n",
    "            elif i==len(layers)-1:\n",
    "                assert layer[\"type\"] == \"output\", \"last layer should be output\"\n",
    "            else:\n",
    "                assert layer[\"type\"] == \"hidden\", \"middle layers should be hidden\"\n",
    "                assert layer[\"activation_function\"] in ACTIVATION_FUNC\n",
    "                \n",
    "            assert type(layer[\"units\"]) == int\n",
    "            \n",
    "            #if the reqularization term is not mentioned --> assign 0.0\n",
    "            if 'regularization_strength' not in layer:\n",
    "                layer['regularization_strength'] = 0.0\n",
    "            # if the keep prob is not mentioned --> assign 1.0\n",
    "            if 'dropout_keep_prob' not in layer:\n",
    "                layer['dropout_keep_prob'] = 1.0\n",
    "            \n",
    "            #validate the regularization and keep prob values\n",
    "            assert type(layer['regularization_strength']) == type(layer['dropout_keep_prob']) == float,\\\n",
    "                    \"regularization_strength and dropout_keep_prob should be float\"\n",
    "                \n",
    "        #done: validation\n",
    "        \n",
    "        #save these info\n",
    "        self.layers = layers\n",
    "        \n",
    "        #keep state variables\n",
    "        self.n_ever_trained = 0\n",
    "        self.total_iterations = 0;\n",
    "        self.parameters_as_json_files = []\n",
    "        \n",
    "        #callable calculational functions to be linked\n",
    "        self.train_accuracy_function = None\n",
    "        self.test_accuracy_function = None\n",
    "        \n",
    "        #costs need to be saved separately per each training\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def absorb_parameters(self,W,B):\n",
    "        self.W=W\n",
    "        self.B=B\n",
    "        \n",
    "    def load_history_state(self,history_state):\n",
    "        history_state = str(history_state)\n",
    "        # only need the random float --> format if underscore and some other things in the given name\n",
    "        if '_' in history_state:\n",
    "            history_state = history_state.split('_')[0]\n",
    "        if not history_state.startswith('0.'):\n",
    "            history_state = '0.' + history_state\n",
    "        #open the jason file and load the parameters\n",
    "        with open(PARAM_FOLDER_NAME + history_state + '_params.json','r') as file:\n",
    "            all_params =  json.load(file)\n",
    "            all_params['W'] = [np.array(w) if w is not None else None for w in all_params['W']]\n",
    "            all_params['B'] = [np.array(b) if b is not None else None for b in all_params['B']]\n",
    "            self.absorb_parameters(all_params['W'],all_params['B'])\n",
    "            print('\\033[31m Reset the parameters according to history state : ' + history_state,'\\033[30m')\n",
    "            self.parameters_as_json_files.append(history_state + '_params.json')\n",
    "        \n",
    "        \n",
    "    def build(self,show=0,weight_init_noise_amplitude=0.01):\n",
    "        # the parameters will be initialized with required dimentions\n",
    "        \n",
    "        self.W = [None] # this array will keep weight matrix  per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.B = [None] # this array will keep bias vector  per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.activation_def = [None] # the pointer to respecting activation function's definition per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.activation_derivative_def = [None] # the pointer to respecting activation function's derivative's  \n",
    "                                                                #definition per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        \n",
    "        self.A = [] # keep all intermidiate activation matrix per each layer (l=1,2,..,L)\n",
    "        self.Z = [] # keep all intermidiate pre-activation matrix per each layer (l=1,2,..,L)\n",
    "        \n",
    "        #initializing\n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            # anything for l=0,1,2,..,L ? \n",
    "            # NO\n",
    "             \n",
    "            \n",
    "            if i: #only start from layer 1,2,..,L\n",
    "                weight_matrix = np.float32(np.random.randn(self.layers[i][\"units\"],self.layers[i-1][\"units\"]) * weight_init_noise_amplitude)\n",
    "                bias_vector = np.zeros((self.layers[i][\"units\"],1),dtype=np.float32)\n",
    "                self.W.append(weight_matrix)\n",
    "                self.B.append(bias_vector)\n",
    "                   \n",
    "               \n",
    "                self.activation_def.append(ACTIVATION_FUNC[self.layers[i][\"activation_function\"]])\n",
    "                self.activation_derivative_def.append(ACTIVATION_FUNC_DERI[self.layers[i][\"activation_function\"]])\n",
    "           \n",
    "            \n",
    "        if show: \n",
    "            print(self.W) \n",
    "            print(self.B)\n",
    "    \n",
    "    def show_image(self,scale=10):\n",
    "        def scaled(x):return scale*x\n",
    "        \n",
    "        layers = self.layers\n",
    "        n_layers = len(layers)\n",
    "        n_neurons = []\n",
    "        for layer in layers:\n",
    "            n_neurons.append(layer['units'])\n",
    "            \n",
    "        # image height and the width need to be estimated\n",
    "        WIDTH_PER_LAYER = scaled(50)\n",
    "        RADIUS_OF_NEURON = scaled(5)\n",
    "        NEURON_SPACING = scaled(2)\n",
    "        width = WIDTH_PER_LAYER * (n_layers-1) + WIDTH_PER_LAYER # added WIDTH_PER_LAYER for padding\n",
    "        height = (2*RADIUS_OF_NEURON + NEURON_SPACING) * max(n_neurons)  +  NEURON_SPACING\n",
    "        \n",
    "        #initialize the canvas (white background)\n",
    "        image = np.zeros((height, width, 3), dtype=np.uint8) + 255\n",
    "        \n",
    "        #drawing the neurons and keeping records on the neuron origin coords\n",
    "        neuron_coords = []\n",
    "        x_pointer = WIDTH_PER_LAYER // 2\n",
    "        movement_per_neuron = 2*RADIUS_OF_NEURON + NEURON_SPACING\n",
    "        for i,layer in enumerate(layers):\n",
    "            y_pointer = NEURON_SPACING + RADIUS_OF_NEURON\n",
    "            # need to vertically center the neurons in hidden layers with less than maximum \n",
    "            extra_space_avail = movement_per_neuron * (max(n_neurons) - n_neurons[i])\n",
    "            y_padding = extra_space_avail // 2\n",
    "            y_pointer+=y_padding\n",
    "            \n",
    "            this_layer_neuron_coords = []\n",
    "            for ni in range(n_neurons[i]):\n",
    "                this_layer_neuron_coords.append((x_pointer,y_pointer))\n",
    "                cv.circle(image,(x_pointer,y_pointer),RADIUS_OF_NEURON,(0,0,0),-1) #r3ki3g skipped scale for thickness\n",
    "                y_pointer+= movement_per_neuron\n",
    "            x_pointer+=WIDTH_PER_LAYER\n",
    "            neuron_coords.append(this_layer_neuron_coords)\n",
    "            \n",
    "        # drawing the lines to represent the connections between neurons\n",
    "        for i in range(1,n_layers):\n",
    "            this_layer_index = i-1\n",
    "            next_layer_index = i\n",
    "            \n",
    "            for start_point in neuron_coords[this_layer_index]:\n",
    "                for end_point in neuron_coords[next_layer_index]:\n",
    "                    cv.line(image,start_point,end_point,(0,0,0),2)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        fig,ax= plt.subplots(1)\n",
    "        ax.imshow(image)\n",
    "        plt.show()\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "    def show_cost_history(self,log=0,grid=1):\n",
    "        PLOT_COLORS = ['blue','red','green','orange']\n",
    "        fig,ax = plt.subplots(1)\n",
    "        pointer = 0\n",
    "        for i in range(len(self.cost_history)):\n",
    "            if not log:\n",
    "                ax.plot(list(range(pointer,pointer:=pointer + len(self.cost_history[i]))),\n",
    "                    self.cost_history[i],\n",
    "                    color = PLOT_COLORS[i%len(PLOT_COLORS)])\n",
    "            else: # need to plot the log of costs\n",
    "                ax.plot(list(range(pointer,pointer:=pointer + len(self.cost_history[i]))),\n",
    "                    np.log(self.cost_history[i]),\n",
    "                    color = PLOT_COLORS[i%len(PLOT_COLORS)])\n",
    "                \n",
    "        #generate a title for the plot\n",
    "        plot_title = f'Cost vs. iterration '\n",
    "        if log:\n",
    "            plot_title += ':: LOG'\n",
    "        ax.set_title(plot_title)\n",
    "        if grid:\n",
    "            ax.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        X = np.array(X,dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        # these are some place holders to keep the arrays in required size\n",
    "        self.A = [None for i in range(len(self.layers))]\n",
    "        self.Z = [None for i in range(len(self.layers))]\n",
    "        self.A[0] = X # the input matrix\n",
    "        \n",
    "       \n",
    "        for i in range(1,len(self.layers)):\n",
    "                    self.Z[i] = self.W[i]@self.A[i-1]+self.B[i]\n",
    "                    self.A[i] = self.activation_def[i](self.Z[i])\n",
    "        # now all the activations in the NN are calculated and stored\n",
    "        \n",
    "        return self.A[len(self.layers) - 1]\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "    def __gen_progress_msg_pieces(self,n_iters,iteration,start_time,X_train,Y_train,X_test,Y_test):\n",
    "            time_now = time.time()\n",
    "            time_remaining = get_nice_time_dura_str ((n_iters - iteration) * (time_now - start_time) / iteration)\n",
    "\n",
    "            #placeholders\n",
    "            test_accuracy_phrase = ''\n",
    "            train_accuracy_phrase = ''\n",
    "            \n",
    "            # accuracy caculations                        \n",
    "            if  self.test_accuracy_function is not None and callable(self.test_accuracy_function):\n",
    "                test_accuracy_phrase = f' test acc : {self.test_accuracy_function(self,X_test,Y_test)}'\n",
    "            if  self.train_accuracy_function is not None and callable(self.train_accuracy_function):\n",
    "                train_accuracy_phrase = f' train acc : {self.train_accuracy_function(self,X_train,Y_train)}'  \n",
    "            return time_remaining,train_accuracy_phrase,test_accuracy_phrase\n",
    "                        \n",
    "        \n",
    "        \n",
    "    def batch_fit(self,X,Y,cost_function='least_square',\n",
    "                  n_iters=1_000,learning_rate=1e-3,\n",
    "                  ADAM=True,beta_1=0.9,beta_2=0.99,adam_epsilon=1e-2,\n",
    "                 X_test=None,\n",
    "                 Y_test=None,\n",
    "                 inline=False):\n",
    "        \n",
    "        assert cost_function in ('least_square','binary_cross_entropy')\n",
    "        Y = np.float32(Y)\n",
    "        X = np.float32(X)\n",
    "      \n",
    "        #warn the user if the training/testing accu. cal func is not set\n",
    "        if self.__warn_if_train_or_test_accuracy_calc_func_not_set(X_test,Y_test):\n",
    "            return\n",
    "        \n",
    "        __printlnend = '\\r' if inline else '\\n'\n",
    "\n",
    "        \n",
    "        if not self.n_ever_trained:\n",
    "            # these are some place holders to keep the arrays in required size\n",
    "            self.A = [None for i in range(len(self.layers))]\n",
    "            self.Z = [None for i in range(len(self.layers))]\n",
    "\n",
    "            self.dZ = [None for i in range(len(self.layers))]\n",
    "            self.dA = [None for i in range(len(self.layers))]\n",
    "            self.dW = [None for i in range(len(self.layers))]\n",
    "            self.dB = [None for i in range(len(self.layers))]\n",
    "            \n",
    "            #placeholders for dropout vectors per each layer\n",
    "            self.dropout_vectors = [None] * (len(self.layers)-1) #for the output layer no need of dropout vector\n",
    "            \n",
    "             # For ADAM, initialize the following\n",
    "            if ADAM:\n",
    "                self.__initialize_RMSprop_with_zeros()\n",
    "                self.__initialize_momentum_with_zeros()\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print(\"Training again..fine tuning of parameters continued from where left at last time...\")\n",
    "\n",
    "        #update the state variable\n",
    "        self.n_ever_trained += 1\n",
    "            \n",
    "        self.A[0] = X # set the input matrix \n",
    "             \n",
    "       \n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.cost_history.append([])\n",
    "\n",
    "        for iteration in range(n_iters): \n",
    "            \n",
    "                # progress message : every 100 iteration except for the first one\n",
    "                if iteration and not iteration%10:\n",
    "                        (time_remaining,\n",
    "                        train_accuracy_phrase,\n",
    "                        test_accuracy_phrase) = self.__gen_progress_msg_pieces(n_iters,iteration,start_time,\n",
    "                                                                       X,Y,X_test,Y_test)\n",
    "                        \n",
    "                        \n",
    "                        print(f\"iteration : {iteration}/{n_iters} ---> ETA : {time_remaining} \" + \\\n",
    "                              train_accuracy_phrase +\\\n",
    "                              test_accuracy_phrase,\n",
    "                              \n",
    "                              end=__printlnend)\n",
    "                # end : progress message\n",
    "            \n",
    "            \n",
    "            \n",
    "                ##### FWD PASS #####\n",
    "                #  r3ki3g : assumes the X in pre-scaled / normalized\n",
    "                \n",
    "                # apply droput settings for the input layer only\n",
    "                should_not_drop = np.random.rand(*self.A[0].shape) < self.layers[0][\"dropout_keep_prob\"]\n",
    "                self.dropout_vectors[0] = should_not_drop\n",
    "                self.A[0] *= should_not_drop /  self.layers[0][\"dropout_keep_prob\"]\n",
    "\n",
    "                # loop though each layer and calculate the pre-activations(Z) and activations(A)\n",
    "                cost_reg_term = 0\n",
    "                for i in range(1,len(self.layers)):\n",
    "                    self.Z[i] = self.W[i]@self.A[i-1]+self.B[i]\n",
    "                    self.A[i] = self.activation_def[i](self.Z[i])\n",
    "                    \n",
    "                    # consider the drop-out settings (except in ouput layer)\n",
    "                    if i != len(self.layers)-1:\n",
    "                        should_not_drop = np.random.rand(*self.A[i].shape) <= self.layers[i][\"dropout_keep_prob\"]\n",
    "                        self.dropout_vectors[i] = should_not_drop\n",
    "                        self.A[i] *= should_not_drop /  self.layers[i][\"dropout_keep_prob\"]\n",
    "                    \n",
    "                    #iclude the regularization term in cost function (temp:cost_reg_term )\n",
    "                    cost_reg_term += np.sum(self.W[i]**2) * self.layers[i][\"regularization_strength\"] / X.shape[1]\n",
    "                    \n",
    "                # now all the activations in the NN are calculated and stored\n",
    "                \n",
    "                \n",
    "                # now calculate the cost at this iteration\n",
    "                if cost_function == 'binary_cross_entropy':\n",
    "                    # 1e-8 added inside log functions to avoid occuring log(0)\n",
    "                    cost = - np.sum((Y * np.log(1e-8 + self.A[len(self.layers) - 1]) + (1.-Y) * np.log(1e-8 + 1. - self.A[len(self.layers) - 1]))) /  X.shape[1] \n",
    "                elif cost_function == 'least_square':\n",
    "                     cost = np.sum((self.A[len(self.layers) - 1] - Y)**2) / X.shape[1]\n",
    "                else:\n",
    "                    raise Exception(\"not implemented yet\")\n",
    "                \n",
    "                #inlcude the regularization term in cost function\n",
    "                cost += cost_reg_term\n",
    "                self.cost_history[-1].append(cost)\n",
    "                # done : cost calc and stroing\n",
    "\n",
    "               \n",
    "\n",
    "                ##### BACK PROP #####\n",
    "                # The order l = L,L-1,L-2,...,3,2,1 (and no 0)\n",
    "                for i in range(len(self.layers) -1 , 0 , -1): # i=0 is excluded  # note :- L is at  len(self.layers) -1 index\n",
    "\n",
    "                    # the output layer pre-activation AKA dZ[L] depends on the choice of cost function\n",
    "                    if i == len(self.layers) -1:\n",
    "                                if cost_function == 'binary_cross_entropy':\n",
    "                                    dL_dA = -(Y/self.A[i]) +((1-Y)/(1-self.A[i]))   \n",
    "                                        \n",
    "#                                        \n",
    "                                elif cost_function == 'least_square':\n",
    "                                    dL_dA = 2 * (self.A[i] - Y)\n",
    "                                    \n",
    "                                   \n",
    "                                    \n",
    "                                # finally we need the dZ (independent from cost function)\n",
    "                                self.dZ[i] = dL_dA * self.activation_derivative_def[i](self.Z[i])\n",
    "\n",
    "\n",
    "\n",
    "                    else: # not the last layer\n",
    "                        self.dZ[i] =  (self.W[i+1].T @ self.dZ[i+1]) * self.activation_derivative_def[i](self.Z[i])\n",
    "                        # dropout affects backpropagation as well\n",
    "                        self.dZ[i] *= self.dropout_vectors[i] / self.layers[i][\"dropout_keep_prob\"]\n",
    "                    \n",
    "                    \n",
    "                    # calculate gradients\n",
    "                    m = X.shape[1]\n",
    "                    self.dW[i] = self.dZ[i]@self.A[i-1].T / m\n",
    "                    self.dB[i] = np.sum(self.dZ[i],axis=1,keepdims=1) / m\n",
    "\n",
    "                    # gradient decent\n",
    "                    # ADAM optimization has to be considered \n",
    "                    \n",
    "                    if 'keep a record of the previous state':\n",
    "                        vdWh = self.vdW[i]\n",
    "                        vdBh = self.vdB[i]\n",
    "                        sdWh = self.sdW[i]\n",
    "                        sdBh = self.sdB[i]\n",
    "                    \n",
    "                    \n",
    "                    self.vdW[i] = beta_1 * self.vdW[i]  + (1. - beta_1) * self.dW[i]\n",
    "                    self.vdB[i] = beta_1 * self.vdB[i]  + (1. - beta_1) * self.dB[i]\n",
    "                    self.sdW[i] = beta_2 * self.sdW[i]  + (1. - beta_2) * self.dW[i]**2\n",
    "                    self.sdB[i] = beta_2 * self.sdB[i]  + (1. - beta_2) * self.dB[i]**2\n",
    "                  \n",
    "\n",
    "                    vdW = self.vdW[i] / (1. - beta_1**(iteration+1))\n",
    "                    vdB = self.vdB[i] / (1. - beta_1**(iteration+1))\n",
    "                    sdW = self.sdW[i] / (1. - beta_2**(iteration+1))\n",
    "                    sdB = self.sdB[i] / (1. - beta_2**(iteration+1))\n",
    "                    \n",
    "                    \n",
    "                    if not 'print the error caused states':\n",
    "                        \n",
    "                        if (np.any(np.isnan(vdW))):\n",
    "                            print(\"nan detedted\",iteration)\n",
    "                            print(\"vdWh\",vdWh)\n",
    "                            print(\"end\")\n",
    "                        if (np.any(np.isnan(vdB))):\n",
    "                            print(\"nan detedted\",iteration)\n",
    "                            print(\"vdBh\",vdBh)\n",
    "                            print(\"end\")\n",
    "                        if (np.any(np.isnan(sdW))):\n",
    "                            print(\"nan detedted\",iteration)\n",
    "                            print(\"sdWh\",sdWh)\n",
    "                            print(\"end\")\n",
    "                        if (np.any(np.isnan(sdB))):\n",
    "                            print(\"nan detedted\",iteration)\n",
    "                            print(\"sdBh\",sdBh)\n",
    "                            print(\"end\")\n",
    "\n",
    "\n",
    "    #                     if iteration<4 or iteration>n_iters-2:\n",
    "    #                         print(\"vdW\",vdW)\n",
    "\n",
    "    #                         print(\"vdB\",vdB)\n",
    "\n",
    "    #                         print(\"sdW\",sdW)\n",
    "    #                         print(\"sdB\",sdB)\n",
    "\n",
    "                    \n",
    "                  \n",
    "                    # the regularization has to be included here\n",
    "                    reg_lambda =  self.layers[i]['regularization_strength']\n",
    "                    \n",
    "#                     self.W[i] = (1 - reg_lambda*learning_rate/m) * self.W[i] \\\n",
    "#                                 - (learning_rate / ( np.sqrt(np.abs(self.sdW[i])) + adam_epsilon) ) * self.vdW[i]\n",
    "#                     self.B[i] = self.B[i] \\\n",
    "#                                 - (learning_rate / (np.sqrt(np.abs(self.sdB[i])) + adam_epsilon) ) * self.vdB[i]\n",
    "                    \n",
    "    \n",
    "                    self.W[i] = (1 - reg_lambda*learning_rate/m) * self.W[i] \\\n",
    "                                - (learning_rate / ( np.sqrt(sdW + 1e-8) + adam_epsilon) ) * vdW\n",
    "                    self.B[i] = self.B[i] \\\n",
    "                                - (learning_rate / ( np.sqrt(sdB + 1e-8) + adam_epsilon) ) * vdB\n",
    "\n",
    "        #update the state variables\n",
    "        self.total_iterations += n_iters\n",
    "        \n",
    "        time_now = time.time()\n",
    "        total_time = get_nice_time_dura_str(time_now - start_time)\n",
    "        _,train_accuracy_phrase,test_accuracy_phrase = self.__gen_progress_msg_pieces(n_iters,iteration,start_time,\n",
    "                                                                                     X,Y,\n",
    "                                                                                     X_test,Y_test)\n",
    "        \n",
    "        print(f\"Training ({self.n_ever_trained}) ended  : n_iters: {n_iters} with learning_rate : {learning_rate}. Time taken : {total_time}\")\n",
    "        print(f'Total summary :: iterrations : {self.total_iterations} {train_accuracy_phrase}{test_accuracy_phrase}')\n",
    "        \n",
    "        #saving the weights in to a file\n",
    "        self.__save_current_learnt_params()\n",
    "        \n",
    "    def history(self):\n",
    "        print('Follwoing history sates are available:')\n",
    "        print(self.parameters_as_json_files)\n",
    "        print(\"Use DNN.load_history_state(history_state) to reset parameters to any state.\")\n",
    "        print(\"Use DNN.back() to ignore last training and return to the previous state.\")\n",
    "        \n",
    "    def back(self):\n",
    "        undo_last__filename = self.parameters_as_json_files[-2]\n",
    "        self.load_history_state(undo_last__filename)\n",
    "        self.parameters_as_json_files.append(undo_last__filename)\n",
    "        print(\"Last training was ignored and returned to the previous state: \" + undo_last__filename)\n",
    "    \n",
    "    def layer(self,name):\n",
    "        selected_layer = None\n",
    "        for layer in self.layers:\n",
    "            if 'name' in layer and layer['name'] == name:\n",
    "                if selected_layer == None:\n",
    "                    selected_layer = layer\n",
    "                else:\n",
    "                    raise Exception(\"Found multiple layers with name : \" +  name)\n",
    "        if selected_layer == None:        \n",
    "            raise Exception(\"No layer with the name : \" + name)\n",
    "        return selected_layer\n",
    "        \n",
    "                \n",
    "            \n",
    "    def __initialize_momentum_with_zeros(self):\n",
    "        self.vdW = [np.zeros(layer_w.shape,dtype=np.float32) if layer_w is not None else None for layer_w in self.W]\n",
    "        self.vdB = [np.zeros(layer_b.shape,dtype=np.float32) if layer_b is not None else None for layer_b in self.B]\n",
    "    def __initialize_RMSprop_with_zeros(self):\n",
    "        self.sdW = [np.zeros(layer_w.shape,dtype=np.float32) if layer_w is not None else None for layer_w in self.W]\n",
    "        self.sdB = [np.zeros(layer_b.shape,dtype=np.float32) if layer_b is not None else None for layer_b in self.B]\n",
    "\n",
    "    def __save_current_learnt_params(self):\n",
    "        #generate a random name for the training-history-point\n",
    "        this_status_name = str(np.random.rand()) + '_params.json'\n",
    "        #make a 'parameters as json' folder if it does not exist already\n",
    "        \n",
    "        if not os.path.isdir(PARAM_FOLDER_NAME):os.mkdir(PARAM_FOLDER_NAME)\n",
    "        with open(PARAM_FOLDER_NAME + this_status_name,'w') as file:\n",
    "            weights = [w.tolist() if not w is None else None for w in self.W]\n",
    "            biases = [b.tolist() if not b is None else None for b in self.B]\n",
    "            all_params = {\"W\":weights,\"B\":biases}\n",
    "            json.dump(all_params,file)\n",
    "            \n",
    "        self.parameters_as_json_files.append(this_status_name)\n",
    "        print(\"saved params to: \" + this_status_name)\n",
    "    def __warn_if_train_or_test_accuracy_calc_func_not_set(self,X_test,Y_test):\n",
    "        if X_test is not None and Y_test is not None:\n",
    "            if self.test_accuracy_function is None:\n",
    "                y_or_n = input(\"\\033[91m self.test_accuracy_function(model,x_test,y_test) is not set.\" + \\\n",
    "                \"Continue? Y/N: \\033[0m\")\n",
    "                if str(y_or_n).lower() != \"y\":\n",
    "                    return 1\n",
    "        return 0\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "###### HELPER FUNCTIONS FOR DNN CLASS ###########    \n",
    "    \n",
    "# define activation functions globally\n",
    "def sigmoid(t):\n",
    "        return 1/ ( 1 + np.exp(-t) )\n",
    "      \n",
    "def relu(t): # from chat gpt : this is safe for any dimension array t\n",
    "    return np.maximum(0,t)\n",
    "\n",
    "def relu_deri(t):\n",
    "    return np.where(t>=0,1.,0.)\n",
    "\n",
    "def sigmoid_deri(t):\n",
    "    return (1-sigmoid(t)) * sigmoid(t)\n",
    "\n",
    "def linear(t):\n",
    "    return t\n",
    "\n",
    "def linear_deri(t):\n",
    "    return 1\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "def tanh_deri(Z):\n",
    "    return 1 - np.tanh(Z)**2\n",
    "                \n",
    "ACTIVATION_FUNC = {\"relu\":relu,\"sigmoid\":sigmoid,\"linear\":linear,\"tanh\":tanh}\n",
    "ACTIVATION_FUNC_DERI = {\"relu\":relu_deri,\"sigmoid\":sigmoid_deri,\"linear\":linear_deri,\"tanh\":tanh_deri}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# helper function for displaying ETA in a nice manner\n",
    "def get_nice_time_dura_str(time_in_secs):\n",
    "    time_in_secs = round(time_in_secs,2)\n",
    "    if time_in_secs >= 60:\n",
    "        n_mins = int(time_in_secs//60)\n",
    "        n_secs = round(time_in_secs%60,2)\n",
    "        return f\"{n_mins} min {n_secs} secs\"\n",
    "    return f\"{time_in_secs} secs\"\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "058a5456",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T08:27:07.096403Z",
     "start_time": "2023-12-06T08:26:12.392811Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10156/1480315139.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m my_test_NN.batch_fit(x_train,y_train,cost_function='least_square',n_iters=1000,learning_rate=1e-2,\n\u001b[0m\u001b[0;32m     78\u001b[0m                     X_test=x_test,Y_test=y_test,)\n\u001b[0;32m     79\u001b[0m \u001b[0mmy_test_NN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_cost_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10156/541018219.py\u001b[0m in \u001b[0;36mbatch_fit\u001b[1;34m(self, X, Y, cost_function, n_iters, learning_rate, ADAM, beta_1, beta_2, adam_epsilon, X_test, Y_test, inline)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;31m#warn the user if the training/testing accu. cal func is not set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__warn_if_train_or_test_accuracy_calc_func_not_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10156/541018219.py\u001b[0m in \u001b[0;36m__warn_if_train_or_test_accuracy_calc_func_not_set\u001b[1;34m(self, X_test, Y_test)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_accuracy_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m                 y_or_n = input(\"\\033[91m self.test_accuracy_function(model,x_test,y_test) is not set.\" + \\\n\u001b[0m\u001b[0;32m    495\u001b[0m                 \"Continue? Y/N: \\033[0m\")\n\u001b[0;32m    496\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_or_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"y\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             )\n\u001b[1;32m-> 1006\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 1e-2\n",
    "def accuracy_calc(model,X,Y):\n",
    "    Y_pred = model.predict(X)\n",
    "    is_Y_pred_correct = np.abs(Y_pred-Y)<THRESHOLD\n",
    "    accuracy = np.sum(is_Y_pred_correct)/is_Y_pred_correct.shape[1]\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "m = 100_000\n",
    "x_all = np.random.random((10,m))\n",
    "W_true = np.array([[1,2,3,4,5,1,2,3,4,5]])\n",
    "b_true = -2\n",
    "y_all = W_true@x_all + b_true\n",
    "\n",
    "# add some noise\n",
    "noise = (2 * np.random.random((1,m)) - 1) * 1e-3\n",
    "y_all += noise\n",
    "\n",
    "# data is ready .. now partition it\n",
    "\n",
    "split_ratio = 0.9 # for training\n",
    "x_train = x_all[:,:int(split_ratio*m)]\n",
    "x_test = x_all[:,int(split_ratio*m):]\n",
    "y_train = y_all[:,:int(split_ratio*m)]\n",
    "y_test = y_all[:,int(split_ratio*m):]\n",
    "\n",
    "\n",
    "n_input = 10    \n",
    "n_output = 1\n",
    "my_test_NN = DNN(layers=[\n",
    "\n",
    "    {\n",
    "        \"type\":\"input\",\n",
    "        \"units\":n_input\n",
    "    },\n",
    "\n",
    "   \n",
    "    {\n",
    "        \"type\":\"hidden\",\n",
    "        \"units\":n_input//2,\n",
    "        \"activation_function\":\"relu\",\n",
    "        \"regularization_strength\":0.0,\n",
    "        \"dropout_keep_prob\":1.0,\n",
    "        \"name\":\"fhl\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"type\":\"hidden\",\n",
    "        \"units\":n_input//4,\n",
    "        \"activation_function\":\"relu\",\n",
    "        \"regularization_strength\":0.0,\n",
    "        \"dropout_keep_prob\":1.0,\n",
    "        \"name\":\"shl\"\n",
    "    },\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    {\n",
    "        \"type\":\"output\",\n",
    "        \"units\":n_output,\n",
    "        \"activation_function\":\"relu\",\n",
    "        \"regularization_strength\":0.0,\n",
    "        \"dropout_keep_prob\":1.0\n",
    "    }\n",
    "\n",
    "])\n",
    "\n",
    "my_test_NN.train_accuracy_function = accuracy_calc\n",
    "# my_test_NN.test_accuracy_function =  accuracy_calc\n",
    "\n",
    "%matplotlib inline\n",
    "my_test_NN.build(weight_init_noise_amplitude=0.01)\n",
    "# my_test_NN.show_image(scale=15)\n",
    "\n",
    "\n",
    "my_test_NN.batch_fit(x_train,y_train,cost_function='least_square',n_iters=1000,learning_rate=1e-2,\n",
    "                    X_test=x_test,Y_test=y_test,)\n",
    "my_test_NN.show_cost_history(log=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d88cca15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T07:39:37.521981Z",
     "start_time": "2023-12-06T07:38:41.605553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training again..fine tuning of parameters continued from where left at last time...\n",
      "Training (2) ended  : n_iters: 1000 with learning_rate : 0.1. Time taken : 55.76 secs\n",
      "Total summary :: iterrations : 2000\n",
      "saved params to: 0.8063853623863088_params.json\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_test_NN.batch_fit(x_train,y_train,cost_function='least_square',n_iters=1000,learning_rate=1e-1)\n",
    "my_test_NN.show_cost_history(log=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
