{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e285cc3c",
   "metadata": {},
   "source": [
    "## What's new in this implementation\n",
    "<font color=\"red\">\n",
    "   In previous DNN implementation I had forgotten  to include the dropout effect in the backpropagation. (Noticed while going through the Coursera lab!)<br/>\n",
    "    Here I correct it.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4971c84a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T15:28:37.536357Z",
     "start_time": "2023-09-25T15:28:35.444511Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import HTML,Latex\n",
    "import os \n",
    "from joblib import Parallel,delayed\n",
    "import json\n",
    "import cv2 as cv\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "585e56ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T15:28:46.084589Z",
     "start_time": "2023-09-25T15:28:45.968503Z"
    }
   },
   "outputs": [],
   "source": [
    "PARAM_FOLDER_NAME = 'parameters_as_json/'\n",
    "class DNN:\n",
    "    def __init__(self,layers):\n",
    "        # layers :  list of dicts containing later information.\n",
    "        assert type(layers) in  (list,tuple)\n",
    "        assert len(layers) >= 2, \"At least an input and an output layer should be there\"\n",
    "        \n",
    "        # validate each layer\n",
    "        for i,layer in enumerate(layers):\n",
    "            if i==0:\n",
    "                assert layer[\"type\"] == \"input\", \"first layer should be input\"\n",
    "            elif i==len(layers)-1:\n",
    "                assert layer[\"type\"] == \"output\", \"last layer should be output\"\n",
    "            else:\n",
    "                assert layer[\"type\"] == \"hidden\", \"middle layers should be hidden\"\n",
    "                assert layer[\"activation_function\"] in ACTIVATION_FUNC\n",
    "                \n",
    "            assert type(layer[\"units\"]) == int\n",
    "            \n",
    "            #if the reqularization term is not mentioned --> assign 0.0\n",
    "            if 'regularization_strength' not in layer:\n",
    "                layer['regularization_strength'] = 0.0\n",
    "            # if the keep prob is not mentioned --> assign 1.0\n",
    "            if 'dropout_keep_prob' not in layer:\n",
    "                layer['dropout_keep_prob'] = 1.0\n",
    "            \n",
    "            #validate the regularization and keep prob values\n",
    "            assert type(layer['regularization_strength']) == type(layer['dropout_keep_prob']) == float,\\\n",
    "                    \"regularization_strength and dropout_keep_prob should be float\"\n",
    "                \n",
    "        #done: validation\n",
    "        \n",
    "        #save these info\n",
    "        self.layers = layers\n",
    "        \n",
    "        #keep state variables\n",
    "        self.ever_trained = 0\n",
    "        self.total_iterations = 0;\n",
    "        self.parameters_as_json_files = []\n",
    "        \n",
    "        #costs need to be saved separately per each training\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def absorb_parameters(self,W,B):\n",
    "        self.W=W\n",
    "        self.B=B\n",
    "        \n",
    "    def load_history_state(self,history_state):\n",
    "        history_state = str(history_state)\n",
    "        # only need the random float --> format if underscore and some other things in the given name\n",
    "        if '_' in history_state:\n",
    "            history_state = history_state.split('_')[0]\n",
    "        if not history_state.startswith('0.'):\n",
    "            history_state = '0.' + history_state\n",
    "        #open the jason file and load the parameters\n",
    "        with open(PARAM_FOLDER_NAME + history_state + '_params.json','r') as file:\n",
    "            all_params =  json.load(file)\n",
    "            all_params['W'] = [np.array(w) if w is not None else None for w in all_params['W']]\n",
    "            all_params['B'] = [np.array(b) if b is not None else None for b in all_params['B']]\n",
    "            self.absorb_parameters(all_params['W'],all_params['B'])\n",
    "            print('\\033[31m Reset the parameters according to history state : ' + history_state,'\\033[30m')\n",
    "            self.parameters_as_json_files.append(history_state + '_params.json')\n",
    "        \n",
    "        \n",
    "    def build(self,show=0,weight_init_noise_amplitude=0.01):\n",
    "        # the parameters will be initialized with required dimentions\n",
    "        \n",
    "        self.W = [None] # this array will keep weight matrix  per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.B = [None] # this array will keep bias vector  per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.activation_def = [None] # the pointer to respecting activation function's definition per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.activation_derivative_def = [None] # the pointer to respecting activation function's derivative's  \n",
    "                                                                #definition per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        \n",
    "        self.A = [] # keep all intermidiate activation matrix per each layer (l=1,2,..,L)\n",
    "        self.Z = [] # keep all intermidiate pre-activation matrix per each layer (l=1,2,..,L)\n",
    "        \n",
    "        #initializing\n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            # anything for l=0,1,2,..,L ? \n",
    "            # NO\n",
    "             \n",
    "            \n",
    "            if i: #only start from layer 1,2,..,L\n",
    "                weight_matrix = np.float32(np.random.randn(self.layers[i][\"units\"],self.layers[i-1][\"units\"]) * weight_init_noise_amplitude)\n",
    "                bias_vector = np.zeros((self.layers[i][\"units\"],1),dtype=np.float32)\n",
    "                self.W.append(weight_matrix)\n",
    "                self.B.append(bias_vector)\n",
    "                   \n",
    "               \n",
    "                self.activation_def.append(ACTIVATION_FUNC[self.layers[i][\"activation_function\"]])\n",
    "                self.activation_derivative_def.append(ACTIVATION_FUNC_DERI[self.layers[i][\"activation_function\"]])\n",
    "           \n",
    "            \n",
    "        if show: \n",
    "            print(self.W) \n",
    "            print(self.B)\n",
    "            \n",
    "            \n",
    "    def show_cost_history(self,log=0):\n",
    "        PLOT_COLORS = ['blue','red','green','orange']\n",
    "        fig,ax = plt.subplots(1)\n",
    "        pointer = 0\n",
    "        for i in range(len(self.cost_history)):\n",
    "            if not log:\n",
    "                ax.plot(list(range(pointer,pointer:=pointer + len(self.cost_history[i]))),\n",
    "                    self.cost_history[i],\n",
    "                    color = PLOT_COLORS[i%len(PLOT_COLORS)])\n",
    "            else: # need to plot the log of costs\n",
    "                ax.plot(list(range(pointer,pointer:=pointer + len(self.cost_history[i]))),\n",
    "                    np.log(self.cost_history[i]),\n",
    "                    color = PLOT_COLORS[i%len(PLOT_COLORS)])\n",
    "                \n",
    "        #generate a title for the plot\n",
    "        plot_title = f'Cost vs. iterration '\n",
    "        if log:\n",
    "            plot_title += ':: LOG'\n",
    "        ax.set_title(plot_title)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        X = np.array(X,dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        # these are some place holders to keep the arrays in required size\n",
    "        self.A = [None for i in range(len(self.layers))]\n",
    "        self.Z = [None for i in range(len(self.layers))]\n",
    "        self.A[0] = X # the input matrix\n",
    "        \n",
    "       \n",
    "        for i in range(1,len(self.layers)):\n",
    "                    self.Z[i] = self.W[i]@self.A[i-1]+self.B[i]\n",
    "                    self.A[i] = self.activation_def[i](self.Z[i])\n",
    "        # now all the activations in the NN are calculated and stored\n",
    "        \n",
    "        return self.A[len(self.layers) - 1]\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    def batch_fit(self,X,Y,cost_function='least_square',\n",
    "                  n_iters=1_000,learning_rate=1e-3,\n",
    "                  ADAM=True,beta_1=0.9,beta_2=0.99,adam_epsilon=1e-2):\n",
    "        \n",
    "        assert cost_function in ('least_square','binary_cross_entropy')\n",
    "        Y = np.float32(Y)\n",
    "        X = np.float32(X)\n",
    "      \n",
    "        \n",
    "        if not self.ever_trained:\n",
    "            # these are some place holders to keep the arrays in required size\n",
    "            self.A = [None for i in range(len(self.layers))]\n",
    "            self.Z = [None for i in range(len(self.layers))]\n",
    "\n",
    "            self.dZ = [None for i in range(len(self.layers))]\n",
    "            self.dA = [None for i in range(len(self.layers))]\n",
    "            self.dW = [None for i in range(len(self.layers))]\n",
    "            self.dB = [None for i in range(len(self.layers))]\n",
    "            \n",
    "            #placeholders for dropout vectors per each layer\n",
    "            self.dropout_vectors = [None] * (len(self.layers)-1) #for the output layer no need of dropout vector\n",
    "            \n",
    "             # For ADAM, initialize the following\n",
    "            if ADAM:\n",
    "                self.__initialize_RMSprop_with_zeros()\n",
    "                self.__initialize_momentum_with_zeros()\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print(\"Training again..fine tuning of parameters continued from where left at last time...\")\n",
    "\n",
    "        #update the state variable\n",
    "        self.ever_trained += 1\n",
    "            \n",
    "        self.A[0] = X # set the input matrix \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.cost_history.append([])\n",
    "        for iteration in range(n_iters): \n",
    "            \n",
    "                # progress message : every 100 iteration except for the first one\n",
    "                if iteration and not iteration%10:\n",
    "                        time_now = time.time()\n",
    "                        time_remaining = get_nice_time_dura_str ((n_iters - iteration) * (time_now - start_time) / iteration)\n",
    "                        print(f\"iteration : {iteration} ---> ETA : {time_remaining} \",end=\"\\r\")\n",
    "                # end : progress message\n",
    "            \n",
    "            \n",
    "            \n",
    "                ##### FWD PASS #####\n",
    "                #  r3ki3g : assumes the X in pre-scaled / normalized\n",
    "                \n",
    "                # apply droput settings for the input layer only\n",
    "                should_not_drop = np.random.rand(*self.A[0].shape) < self.layers[0][\"dropout_keep_prob\"]\n",
    "                self.dropout_vectors[0] = should_not_drop\n",
    "                self.A[0] *= should_not_drop /  self.layers[0][\"dropout_keep_prob\"]\n",
    "\n",
    "                # loop though each layer and calculate the pre-activations(Z) and activations(A)\n",
    "                cost_reg_term = 0\n",
    "                for i in range(1,len(self.layers)):\n",
    "                    self.Z[i] = self.W[i]@self.A[i-1]+self.B[i]\n",
    "                    self.A[i] = self.activation_def[i](self.Z[i])\n",
    "                    \n",
    "                    # consider the drop-out settings (except in ouput layer)\n",
    "                    if i != len(self.layers)-1:\n",
    "                        should_not_drop = np.random.rand(*self.A[i].shape) <= self.layers[i][\"dropout_keep_prob\"]\n",
    "                        self.dropout_vectors[i] = should_not_drop\n",
    "                        self.A[i] *= should_not_drop /  self.layers[i][\"dropout_keep_prob\"]\n",
    "                    \n",
    "                    #iclude the regularization term in cost function (temp:cost_reg_term )\n",
    "                    cost_reg_term += np.sum(self.W[i]**2) * self.layers[i][\"regularization_strength\"] / X.shape[1]\n",
    "                    \n",
    "                # now all the activations in the NN are calculated and stored\n",
    "                \n",
    "                \n",
    "                # now calculate the cost at this iteration\n",
    "                if cost_function == 'binary_cross_entropy':\n",
    "                    # 1e-8 added inside log functions to avoid occuring log(0)\n",
    "                    cost = - np.sum((Y * np.log(1e-8 + self.A[len(self.layers) - 1]) + (1.-Y) * np.log(1e-8 + 1. - self.A[len(self.layers) - 1]))) /  X.shape[1] \n",
    "                elif cost_function == 'least_square':\n",
    "                     cost = np.sum((self.A[len(self.layers) - 1] - Y)**2) / X.shape[1]\n",
    "                else:\n",
    "                    raise Exception(\"not implemented yet\")\n",
    "                \n",
    "                #inlcude the regularization term in cost function\n",
    "                cost += cost_reg_term\n",
    "                self.cost_history[-1].append(cost)\n",
    "                # done : cost calc and stroing\n",
    "\n",
    "               \n",
    "\n",
    "                ##### BACK PROP #####\n",
    "                # The order l = L,L-1,L-2,...,3,2,1 (and no 0)\n",
    "                for i in range(len(self.layers) -1 , 0 , -1): # i=0 is excluded  # note :- L is at  len(self.layers) -1 index\n",
    "\n",
    "                    # the output layer pre-activation AKA dZ[L] depends on the choice of cost function\n",
    "                    if i == len(self.layers) -1:\n",
    "                                if cost_function == 'binary_cross_entropy':\n",
    "                                    dL_dA = -(Y/self.A[i]) +((1-Y)/(1-self.A[i]))   \n",
    "                                        \n",
    "#                                        \n",
    "                                elif cost_function == 'least_square':\n",
    "                                    dL_dA = 2 * (self.A[i] - Y)\n",
    "                                    \n",
    "                                   \n",
    "                                    \n",
    "                                # finally we need the dZ (independent from cost function)\n",
    "                                self.dZ[i] = dL_dA * self.activation_derivative_def[i](self.Z[i])\n",
    "\n",
    "\n",
    "\n",
    "                    else: # not the last layer\n",
    "                        self.dZ[i] =  (self.W[i+1].T @ self.dZ[i+1]) * self.activation_derivative_def[i](self.Z[i])\n",
    "                        # dropout affects backpropagation as well\n",
    "                        self.dZ[i] *= self.dropout_vectors[i] / self.layers[i][\"dropout_keep_prob\"]\n",
    "                    \n",
    "                    \n",
    "                    # calculate gradients\n",
    "                    m = X.shape[1]\n",
    "                    self.dW[i] = self.dZ[i]@self.A[i-1].T / m\n",
    "                    self.dB[i] = np.sum(self.dZ[i],axis=1,keepdims=1) / m\n",
    "\n",
    "                    # gradient decent\n",
    "                    # ADAM optimization has to be considered \n",
    "                    \n",
    "                    if 'keep a record of the previous state':\n",
    "                        vdWh = self.vdW[i]\n",
    "                        vdBh = self.vdB[i]\n",
    "                        sdWh = self.sdW[i]\n",
    "                        sdBh = self.sdB[i]\n",
    "                    \n",
    "                    \n",
    "                    self.vdW[i] = beta_1 * self.vdW[i]  + (1. - beta_1) * self.dW[i]\n",
    "                    self.vdB[i] = beta_1 * self.vdB[i]  + (1. - beta_1) * self.dB[i]\n",
    "                    self.sdW[i] = beta_2 * self.sdW[i]  + (1. - beta_2) * self.dW[i]**2\n",
    "                    self.sdB[i] = beta_2 * self.sdB[i]  + (1. - beta_2) * self.dB[i]**2\n",
    "                  \n",
    "\n",
    "                    vdW = self.vdW[i] / (1. - beta_1**(iteration+1))\n",
    "                    vdB = self.vdB[i] / (1. - beta_1**(iteration+1))\n",
    "                    sdW = self.sdW[i] / (1. - beta_2**(iteration+1))\n",
    "                    sdB = self.sdB[i] / (1. - beta_2**(iteration+1))\n",
    "                    \n",
    "                    \n",
    "                    if not 'print the error caused states':\n",
    "                        \n",
    "                        if (np.any(np.isnan(vdW))):\n",
    "                            print(\"nan detedted\",iteration)\n",
    "                            print(\"vdWh\",vdWh)\n",
    "                            print(\"end\")\n",
    "                        if (np.any(np.isnan(vdB))):\n",
    "                            print(\"nan detedted\",iteration)\n",
    "                            print(\"vdBh\",vdBh)\n",
    "                            print(\"end\")\n",
    "                        if (np.any(np.isnan(sdW))):\n",
    "                            print(\"nan detedted\",iteration)\n",
    "                            print(\"sdWh\",sdWh)\n",
    "                            print(\"end\")\n",
    "                        if (np.any(np.isnan(sdB))):\n",
    "                            print(\"nan detedted\",iteration)\n",
    "                            print(\"sdBh\",sdBh)\n",
    "                            print(\"end\")\n",
    "\n",
    "\n",
    "    #                     if iteration<4 or iteration>n_iters-2:\n",
    "    #                         print(\"vdW\",vdW)\n",
    "\n",
    "    #                         print(\"vdB\",vdB)\n",
    "\n",
    "    #                         print(\"sdW\",sdW)\n",
    "    #                         print(\"sdB\",sdB)\n",
    "\n",
    "                    \n",
    "                  \n",
    "                    # the regularization has to be included here\n",
    "                    reg_lambda =  self.layers[i]['regularization_strength']\n",
    "                    \n",
    "#                     self.W[i] = (1 - reg_lambda*learning_rate/m) * self.W[i] \\\n",
    "#                                 - (learning_rate / ( np.sqrt(np.abs(self.sdW[i])) + adam_epsilon) ) * self.vdW[i]\n",
    "#                     self.B[i] = self.B[i] \\\n",
    "#                                 - (learning_rate / (np.sqrt(np.abs(self.sdB[i])) + adam_epsilon) ) * self.vdB[i]\n",
    "                    \n",
    "    \n",
    "                    self.W[i] = (1 - reg_lambda*learning_rate/m) * self.W[i] \\\n",
    "                                - (learning_rate / ( np.sqrt(sdW + 1e-8) + adam_epsilon) ) * vdW\n",
    "                    self.B[i] = self.B[i] \\\n",
    "                                - (learning_rate / ( np.sqrt(sdB + 1e-8) + adam_epsilon) ) * vdB\n",
    "\n",
    "        #update the state variables\n",
    "        self.total_iterations += n_iters\n",
    "        \n",
    "        time_now = time.time()\n",
    "        total_time = get_nice_time_dura_str(time_now - start_time)\n",
    "        print(f\"Training ({self.ever_trained}) ended  : n_iters: {n_iters} with learning_rate : {learning_rate}. Time taken : {total_time}\")\n",
    "        print(f'Total summary :: iterrations : {self.total_iterations}')\n",
    "        \n",
    "        #saving the weights in to a file\n",
    "        self.__save_current_learnt_params()\n",
    "        \n",
    "    def history(self):\n",
    "        print('Follwoing history sates are available:')\n",
    "        print(self.parameters_as_json_files)\n",
    "        print(\"Use DNN.load_history_state(history_state) to reset parameters to any state.\")\n",
    "        print(\"Use DNN.back() to ignore last training and return to the previous state.\")\n",
    "        \n",
    "    def back(self):\n",
    "        undo_last__filename = self.parameters_as_json_files[-2]\n",
    "        self.load_history_state(undo_last__filename)\n",
    "        self.parameters_as_json_files.append(undo_last__filename)\n",
    "        print(\"Last training was ignored and returned to the previous state: \" + undo_last__filename)\n",
    "    \n",
    "    def layer(self,name):\n",
    "        selected_layer = None\n",
    "        for layer in self.layers:\n",
    "            if 'name' in layer and layer['name'] == name:\n",
    "                if selected_layer == None:\n",
    "                    selected_layer = layer\n",
    "                else:\n",
    "                    raise Exception(\"Found multiple layers with name : \" +  name)\n",
    "        if selected_layer == None:        \n",
    "            raise Exception(\"No layer with the name : \" + name)\n",
    "        return selected_layer\n",
    "        \n",
    "                \n",
    "            \n",
    "    def __initialize_momentum_with_zeros(self):\n",
    "        self.vdW = [np.zeros(layer_w.shape,dtype=np.float32) if layer_w is not None else None for layer_w in self.W]\n",
    "        self.vdB = [np.zeros(layer_b.shape,dtype=np.float32) if layer_b is not None else None for layer_b in self.B]\n",
    "    def __initialize_RMSprop_with_zeros(self):\n",
    "        self.sdW = [np.zeros(layer_w.shape,dtype=np.float32) if layer_w is not None else None for layer_w in self.W]\n",
    "        self.sdB = [np.zeros(layer_b.shape,dtype=np.float32) if layer_b is not None else None for layer_b in self.B]\n",
    "\n",
    "    def __save_current_learnt_params(self):\n",
    "        #generate a random name for the training-history-point\n",
    "        this_status_name = str(np.random.rand()) + '_params.json'\n",
    "        #make a 'parameters as json' folder if it does not exist already\n",
    "        \n",
    "        if not os.path.isdir(PARAM_FOLDER_NAME):os.mkdir(PARAM_FOLDER_NAME)\n",
    "        with open(PARAM_FOLDER_NAME + this_status_name,'w') as file:\n",
    "            weights = [w.tolist() if not w is None else None for w in self.W]\n",
    "            biases = [b.tolist() if not b is None else None for b in self.B]\n",
    "            all_params = {\"W\":weights,\"B\":biases}\n",
    "            json.dump(all_params,file)\n",
    "            \n",
    "        self.parameters_as_json_files.append(this_status_name)\n",
    "        print(\"saved params to: \" + this_status_name)\n",
    "        \n",
    "                      \n",
    "\n",
    "    \n",
    "    \n",
    "###### HELPER FUNCTIONS FOR DNN CLASS ###########    \n",
    "    \n",
    "# define activation functions globally\n",
    "def sigmoid(t):\n",
    "        return 1/ ( 1 + np.exp(-t) )\n",
    "      \n",
    "def relu(t): # from chat gpt : this is safe for any dimension array t\n",
    "    return np.maximum(0,t)\n",
    "\n",
    "def relu_deri(t):\n",
    "    return np.where(t>=0,1.,0.)\n",
    "\n",
    "def sigmoid_deri(t):\n",
    "    return (1-sigmoid(t)) * sigmoid(t)\n",
    "\n",
    "def linear(t):\n",
    "    return t\n",
    "\n",
    "def linear_deri(t):\n",
    "    return 1\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "def tanh_deri(Z):\n",
    "    return 1 - np.tanh(Z)**2\n",
    "                \n",
    "ACTIVATION_FUNC = {\"relu\":relu,\"sigmoid\":sigmoid,\"linear\":linear,\"tanh\":tanh}\n",
    "ACTIVATION_FUNC_DERI = {\"relu\":relu_deri,\"sigmoid\":sigmoid_deri,\"linear\":linear_deri,\"tanh\":tanh_deri}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# helper function for displaying ETA in a nice manner\n",
    "def get_nice_time_dura_str(time_in_secs):\n",
    "    time_in_secs = round(time_in_secs,2)\n",
    "    if time_in_secs >= 60:\n",
    "        n_mins = int(time_in_secs//60)\n",
    "        n_secs = round(time_in_secs%60,2)\n",
    "        return f\"{n_mins} min {n_secs} secs\"\n",
    "    return f\"{time_in_secs} secs\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76168229",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T15:29:56.247633Z",
     "start_time": "2023-09-25T15:28:48.008556Z"
    }
   },
   "outputs": [],
   "source": [
    "# data set processing\n",
    "basedir = '''D:/ENTC/PROJECTS/digit-recognition-with-convolution-from-scratch/MNIST Dataset JPG format/MNIST Dataset JPG format/'''\n",
    "train_basedir = basedir + '''training/'''\n",
    "\n",
    "N_EXAMPLES_PER_CLASS = 4000\n",
    "\n",
    "\n",
    "all_training_example = []\n",
    "for i in range(10):\n",
    "    # integer = i\n",
    "    this_file_path = train_basedir  +  str(i) + '/'\n",
    "    files = os.listdir(this_file_path)\n",
    "    assert len(files)>=N_EXAMPLES_PER_CLASS\n",
    "    files = files[:N_EXAMPLES_PER_CLASS]\n",
    "    \n",
    "    all_images_for_class_i  = np.zeros((28*28,N_EXAMPLES_PER_CLASS))\n",
    "    for j,filepath in enumerate(files):\n",
    "        img = cv.imread(this_file_path + filepath,0) #gray scale:: each image ==> (2d array)\n",
    "        assert img is not None\n",
    "        img = cv.GaussianBlur(img,(11,11),4)\n",
    "        img_flattened = img.reshape(-1)\n",
    "        all_images_for_class_i[:,j] = img_flattened\n",
    "    all_training_example.append(all_images_for_class_i)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcde3b80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T15:29:56.558683Z",
     "start_time": "2023-09-25T15:29:56.537596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 4000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_training_example[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b0115a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T15:30:27.596260Z",
     "start_time": "2023-09-25T15:30:24.850301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d4db6529a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# making entire training set\n",
    "X_train = np.concatenate((all_training_example[0],\n",
    "                         all_training_example[1],\n",
    "                         all_training_example[2],\n",
    "                         all_training_example[3],\n",
    "                         all_training_example[4],\n",
    "                         all_training_example[5],\n",
    "                         all_training_example[6],\n",
    "                         all_training_example[7],\n",
    "                         all_training_example[8],\n",
    "                         all_training_example[9]),axis=1)\n",
    "\n",
    "X_train /= 255 # to normalize\n",
    "\n",
    "\n",
    "def onehot(index,size):\n",
    "    vector = np.zeros((size,1))\n",
    "    vector[index,0] = 1\n",
    "    return vector\n",
    "\n",
    "all_training_onehot_y = []\n",
    "for i in range(10):\n",
    "    this_onehot_vector = onehot(i,10)\n",
    "    all_training_onehot_y.append(np.tile(this_onehot_vector,(1,N_EXAMPLES_PER_CLASS)))\n",
    "\n",
    "#print a test\n",
    "print(all_training_onehot_y[7][:,1])\n",
    "\n",
    "Y_train = np.concatenate((all_training_onehot_y[0],\n",
    "                         all_training_onehot_y[1],\n",
    "                         all_training_onehot_y[2],\n",
    "                         all_training_onehot_y[3],\n",
    "                         all_training_onehot_y[4],\n",
    "                         all_training_onehot_y[5],\n",
    "                         all_training_onehot_y[6],\n",
    "                         all_training_onehot_y[7],\n",
    "                         all_training_onehot_y[8],\n",
    "                         all_training_onehot_y[9]),axis=1)\n",
    "\n",
    "fig,ax = plt.subplots(2)\n",
    "ax[0].imshow(X_train)\n",
    "ax[1].imshow(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c3398",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T15:29:46.633669Z",
     "start_time": "2023-09-24T15:29:35.835470Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_input = 784     # =  28 * 28\n",
    "n_output = 10\n",
    "int_recog_NN = DNN(layers=[\n",
    "\n",
    "    {\n",
    "        \"type\":\"input\",\n",
    "        \"units\":n_input\n",
    "    },\n",
    "\n",
    "   \n",
    "    {\n",
    "        \"type\":\"hidden\",\n",
    "        \"units\":100,\n",
    "        \"activation_function\":\"tanh\",\n",
    "        \"regularization_strength\":1e2,\n",
    "        \"dropout_keep_prob\":1.0,\n",
    "        \"name\":\"fhl\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"type\":\"hidden\",\n",
    "        \"units\":20,\n",
    "        \"activation_function\":\"tanh\",\n",
    "        \"regularization_strength\":1e2,\n",
    "        \"dropout_keep_prob\":1.0,\n",
    "        \"name\":\"shl\"\n",
    "    },\n",
    "    \n",
    "#     {\n",
    "#         \"type\":\"hidden\",\n",
    "#         \"units\":n_input//8,\n",
    "#         \"activation_function\":\"relu\",\n",
    "#         \"regularization_strength\":0.0,\n",
    "#         \"dropout_keep_prob\":1.0,\n",
    "#         \"name\":\"thl\"\n",
    "#     },\n",
    "  \n",
    "    \n",
    "    {\n",
    "        \"type\":\"output\",\n",
    "        \"units\":n_output,\n",
    "        \"activation_function\":\"sigmoid\",\n",
    "        \"regularization_strength\":0.0,\n",
    "        \"dropout_keep_prob\":1.0\n",
    "    }\n",
    "\n",
    "])\n",
    "\n",
    "int_recog_NN.build(weight_init_noise_amplitude=0.01)\n",
    "\n",
    "int_recog_NN.batch_fit(X_train,\n",
    "                       Y_train,\n",
    "                       cost_function=('least_square','binary_cross_entropy')[0],\n",
    "                       n_iters=1_0,\n",
    "                       learning_rate=1e-4,beta_1=0.9,beta_2=0.99)\n",
    "\n",
    "%matplotlib inline\n",
    "int_recog_NN.show_cost_history(log=1)\n",
    "\n",
    "display(HTML(f' <font size=\"5\" color=\"red\"> Final cost: {int_recog_NN.cost_history[-1][-1]}</font>'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428db63b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T15:30:23.198493Z",
     "start_time": "2023-09-24T15:30:23.192218Z"
    }
   },
   "outputs": [],
   "source": [
    "int_recog_NN.history()\n",
    "# int_recog_NN.back()\n",
    "int_recog_NN.layer('fhl')['regularization_strength']=1e1\n",
    "int_recog_NN.layer('shl')['regularization_strength']=1e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc8739a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T15:34:53.048293Z",
     "start_time": "2023-09-24T15:32:43.416231Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "int_recog_NN.batch_fit(X_train,\n",
    "                       Y_train,\n",
    "                       cost_function=('least_square','binary_cross_entropy')[0],\n",
    "                       n_iters=1_00,\n",
    "                       learning_rate=5e-2)\n",
    "\n",
    "%matplotlib inline\n",
    "int_recog_NN.show_cost_history(log=1)\n",
    "\n",
    "display(HTML(f'<font size=\"5\" color=\"red\"> Final cost : {int_recog_NN.cost_history[-1][-1]}</font>'))\n",
    "int_recog_NN.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bbd9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T10:02:50.496268Z",
     "start_time": "2023-09-24T10:02:50.484628Z"
    }
   },
   "outputs": [],
   "source": [
    "int_recog_NN.history()\n",
    "int_recog_NN.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8d4f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T11:46:51.561343Z",
     "start_time": "2023-09-24T11:46:50.667530Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# cross validation\n",
    "\n",
    "# data set processing\n",
    "basedir = '''D:/ENTC/PROJECTS/digit-recognition-with-convolution-from-scratch/MNIST Dataset JPG format/MNIST Dataset JPG format/'''\n",
    "dev_basedir = basedir + '''testing/'''\n",
    "\n",
    "N_EXAMPLES_PER_CLASS = 100\n",
    "\n",
    "\n",
    "all_dev_example = []\n",
    "for i in range(10):\n",
    "    # integer = i\n",
    "    this_file_path = dev_basedir + str(i) + '/'\n",
    "    files = os.listdir(this_file_path)\n",
    "    assert len(files) >= N_EXAMPLES_PER_CLASS\n",
    "    files = files[:N_EXAMPLES_PER_CLASS]\n",
    "\n",
    "    all_images_for_class_i = np.zeros((28*28, N_EXAMPLES_PER_CLASS))\n",
    "    for j, filepath in enumerate(files):\n",
    "        # gray scale:: each image ==> (2d array)\n",
    "        img = cv.imread(this_file_path + filepath, 0)\n",
    "        assert img is not None\n",
    "        img = cv.GaussianBlur(img,(11,11),4)\n",
    "        img_flattened = img.reshape(-1)\n",
    "        all_images_for_class_i[:, j] = img_flattened\n",
    "    all_dev_example.append(all_images_for_class_i)\n",
    "\n",
    "\n",
    "# making entire training set\n",
    "X_dev = np.concatenate((all_dev_example[0],\n",
    "                        all_dev_example[1],\n",
    "                        all_dev_example[2],\n",
    "                        all_dev_example[3],\n",
    "                        all_dev_example[4],\n",
    "                        all_dev_example[5],\n",
    "                        all_dev_example[6],\n",
    "                        all_dev_example[7],\n",
    "                        all_dev_example[8],\n",
    "                        all_dev_example[9]), axis=1)\n",
    "\n",
    "X_dev /= 255 # normalize\n",
    "\n",
    "all_training_onehot_y = []\n",
    "for i in range(10):\n",
    "    this_onehot_vector = onehot(i, 10)\n",
    "    all_training_onehot_y.append(\n",
    "        np.tile(this_onehot_vector, (1, N_EXAMPLES_PER_CLASS)))\n",
    "\n",
    "# print a test\n",
    "print(all_training_onehot_y[7][:, 1])\n",
    "\n",
    "Y_dev = np.concatenate((all_training_onehot_y[0],\n",
    "                        all_training_onehot_y[1],\n",
    "                        all_training_onehot_y[2],\n",
    "                        all_training_onehot_y[3],\n",
    "                        all_training_onehot_y[4],\n",
    "                        all_training_onehot_y[5],\n",
    "                        all_training_onehot_y[6],\n",
    "                        all_training_onehot_y[7],\n",
    "                        all_training_onehot_y[8],\n",
    "                        all_training_onehot_y[9]), axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "ax[0].imshow(X_dev)\n",
    "ax[1].imshow(Y_dev)\n",
    "\n",
    "pred_dev = int_recog_NN.predict(X_dev)\n",
    "\n",
    "\n",
    "dev_cost = np.sum((pred_dev - Y_dev)**2) / X_dev.shape[1]\n",
    "display(\n",
    "    HTML(f'<font color=\"red\" size=\"5\">Cross validation cost: {dev_cost}</font>'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb02f6e",
   "metadata": {},
   "source": [
    "### See the wrongly predicted from testings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e41b68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T18:04:31.989043Z",
     "start_time": "2023-09-22T18:04:31.038657Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testing_path = 'D:/ENTC/PROJECTS/digit-recognition-with-convolution-from-scratch/MNIST Dataset JPG format/MNIST Dataset JPG format/testing/9/'\n",
    "\n",
    "allimg = []\n",
    "\n",
    "allfiles = os.listdir(testing_path)\n",
    "for filepath in allfiles:\n",
    "    \n",
    "    img = cv.imread(testing_path +filepath ,0)\n",
    "    assert img is not None\n",
    "    img = cv.GaussianBlur(img,(11,11),4)\n",
    "\n",
    "    imgf = img.reshape(-1,1)\n",
    "  \n",
    "    allimg.append(imgf)\n",
    "\n",
    "\n",
    "allimg = np.array(allimg).T.reshape(784,-1)\n",
    "\n",
    "\n",
    "preds = int_recog_NN.predict(allimg)\n",
    "\n",
    "labels = np.argmax(preds,axis=0)\n",
    "np. set_printoptions(threshold=np.inf)\n",
    "pprint.pprint(labels)\n",
    "\n",
    "\n",
    "#analysing :: manual\n",
    "wrongpredicted = []\n",
    "for i,label in enumerate(labels):\n",
    "    if label != 9:\n",
    "        wrongpredicted.append(i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdafcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T18:05:22.545661Z",
     "start_time": "2023-09-22T18:05:02.592059Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for wp in wrongpredicted:\n",
    "    img = allimg[:,wp].reshape(28,28)\n",
    "    plt.imshow(img,cmap=\"gray\")\n",
    "    plt.title(f\"predicted as : {labels[wp]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319dd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
