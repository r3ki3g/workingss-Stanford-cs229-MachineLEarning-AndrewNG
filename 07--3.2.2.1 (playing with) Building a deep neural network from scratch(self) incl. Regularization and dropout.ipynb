{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0771f302",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T17:53:49.278967Z",
     "start_time": "2023-08-25T17:53:48.878675Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import HTML,Latex\n",
    "import os \n",
    "from joblib import Parallel,delayed\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04cda092",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T17:53:50.350363Z",
     "start_time": "2023-08-25T17:53:50.308829Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self,layers):\n",
    "        # layers :  list of dicts containing later information.\n",
    "        assert type(layers) in  (list,tuple)\n",
    "        assert len(layers) >= 2, \"At least an input and an output layer should be there\"\n",
    "        \n",
    "        # validate each layer\n",
    "        for i,layer in enumerate(layers):\n",
    "            if i==0:\n",
    "                assert layer[\"type\"] == \"input\", \"first layer should be input\"\n",
    "            elif i==len(layers)-1:\n",
    "                assert layer[\"type\"] == \"output\", \"last layer should be output\"\n",
    "            else:\n",
    "                assert layer[\"type\"] == \"hidden\", \"middle layers should be hidden\"\n",
    "                assert layer[\"activation_function\"] in ACTIVATION_FUNC\n",
    "                \n",
    "            assert type(layer[\"units\"]) == int\n",
    "            \n",
    "            #if the reqularization term is not mentioned --> assign 0.0\n",
    "            if 'regularization_strength' not in layer:\n",
    "                layer['regularization_strength'] = 0.0\n",
    "            # if the keep prob is not mentioned --> assign 1.0\n",
    "            if 'dropout_keep_prob' not in layer:\n",
    "                layer['dropout_keep_prob'] = 1.0\n",
    "            \n",
    "            #validate the regularization and keep prob values\n",
    "            assert type(layer['regularization_strength']) == type(layer['dropout_keep_prob']) == float,\\\n",
    "                    \"regularization_strength and dropout_keep_prob should be float\"\n",
    "                \n",
    "        #done: validation\n",
    "        \n",
    "        #save these info\n",
    "        self.layers = layers\n",
    "        \n",
    "        #keep state variables\n",
    "        self.ever_trained = 0\n",
    "        \n",
    "        #costs need to be saved separately per each training\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def absorb_parameters(self,W,B):\n",
    "        self.W=W\n",
    "        self.B=B\n",
    "        \n",
    "    def build(self,show=0):\n",
    "        # the parameters will be initialized with required dimentions\n",
    "        \n",
    "        self.W = [None] # this array will keep weight matrix  per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.B = [None] # this array will keep bias vector  per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.activation_def = [None] # the pointer to respecting activation function's definition per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.activation_derivative_def = [None] # the pointer to respecting activation function's derivative's  \n",
    "                                                                #definition per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        \n",
    "        self.A = [] # keep all intermidiate activation matrix per each layer (l=1,2,..,L)\n",
    "        self.Z = [] # keep all intermidiate pre-activation matrix per each layer (l=1,2,..,L)\n",
    "        \n",
    "        #initializing\n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            # anything for l=0,1,2,..,L ? \n",
    "            # NO\n",
    "             \n",
    "            \n",
    "            if i: #only start from layer 1,2,..,L\n",
    "                weight_matrix = np.random.randn(self.layers[i][\"units\"],self.layers[i-1][\"units\"]) * 0.01\n",
    "                bias_vector = np.zeros((self.layers[i][\"units\"],1))\n",
    "                self.W.append(weight_matrix)\n",
    "                self.B.append(bias_vector)\n",
    "                   \n",
    "               \n",
    "                self.activation_def.append(ACTIVATION_FUNC[self.layers[i][\"activation_function\"]])\n",
    "                self.activation_derivative_def.append(ACTIVATION_FUNC_DERI[self.layers[i][\"activation_function\"]])\n",
    "           \n",
    "            \n",
    "        if show: \n",
    "            print(self.W) \n",
    "            print(self.B)\n",
    "    def show_cost_history(self):\n",
    "        PLOT_COLORS = ['blue','red','green','orange']\n",
    "        fig,ax = plt.subplots(1)\n",
    "        pointer = 0\n",
    "        for i,cost_histry_batch in enumerate(self.cost_history):\n",
    "            ax.plot(list(range(pointer,pointer:=pointer + len(self.cost_history[i]))),\n",
    "                    self.cost_history[i],\n",
    "                    color=PLOT_COLORS[i%len(PLOT_COLORS)])\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        # these are some place holders to keep the arrays in required size\n",
    "        self.A = [None for i in range(len(self.layers))]\n",
    "        self.Z = [None for i in range(len(self.layers))]\n",
    "        self.A[0] = X # the input matrix\n",
    "        \n",
    "       \n",
    "        for i in range(1,len(self.layers)):\n",
    "                    self.Z[i] = self.W[i]@self.A[i-1]+self.B[i]\n",
    "                    self.A[i] = self.activation_def[i](self.Z[i])\n",
    "        # now all the activations in the NN are calculated and stored\n",
    "        \n",
    "        return self.A[len(self.layers) - 1]\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "    def batch_fit(self,X,Y,cost_function='least_square',n_iters=1_000,learning_rate=1e-3):\n",
    "        assert cost_function in ('least_square','binary_cross_entropy')\n",
    "        Y = np.float64(Y)\n",
    "        \n",
    "      \n",
    "        \n",
    "        if not self.ever_trained:\n",
    "            # these are some place holders to keep the arrays in required size\n",
    "            self.A = [None for i in range(len(self.layers))]\n",
    "            self.Z = [None for i in range(len(self.layers))]\n",
    "\n",
    "            self.dZ = [None for i in range(len(self.layers))]\n",
    "            self.dA = [None for i in range(len(self.layers))]\n",
    "            self.dW = [None for i in range(len(self.layers))]\n",
    "            self.dB = [None for i in range(len(self.layers))]\n",
    "            \n",
    "            #update the state variable\n",
    "            self.ever_trained += 1\n",
    "        else:\n",
    "            print(\"Training again..fine tuning of parameters continued from where left at last time...\")\n",
    "\n",
    "        \n",
    "        self.A[0] = X # set the input matrix \n",
    "        # apply droput settings for the input layer only\n",
    "        should_dropout = np.random.rand(*self.A[0].shape) < self.layers[0][\"dropout_keep_prob\"]\n",
    "        self.A[0] *= should_dropout /  self.layers[0][\"dropout_keep_prob\"]\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.cost_history.append([])\n",
    "        start_time = time.time()\n",
    "        for iteration in range(n_iters): \n",
    "            \n",
    "                # progress message : every 100 iteration except for the first one\n",
    "                if iteration and not iteration%100:\n",
    "                        time_now = time.time()\n",
    "                        time_remaining = get_nice_time_dura_str ((n_iters - iteration) * (time_now - start_time) / iteration)\n",
    "                        print(f\"iteration : {iteration} ---> ETA : {time_remaining} \",end=\"\\r\")\n",
    "            \n",
    "            \n",
    "                ##### FWD PASS #####\n",
    "                #  r3ki3g : assumes the X in pre-scaled / normalized\n",
    "                \n",
    "                \n",
    "                # loop though each layer and calculate the pre-activations(Z) and activations(A)\n",
    "                cost_reg_term = 0\n",
    "                for i in range(1,len(self.layers)):\n",
    "                    self.Z[i] = self.W[i]@self.A[i-1]+self.B[i]\n",
    "                    self.A[i] = self.activation_def[i](self.Z[i])\n",
    "                    \n",
    "                    # consider the drop-out settings\n",
    "                    should_dropout = np.random.rand(*self.A[i].shape) < self.layers[i][\"dropout_keep_prob\"]\n",
    "                    self.A[i] *= should_dropout /  self.layers[i][\"dropout_keep_prob\"]\n",
    "                    \n",
    "                    #iclude the regularization term in cost function (temp:cost_reg_term )\n",
    "                    cost_reg_term += np.sum(self.W[i]**2) * self.layers[i][\"regularization_strength\"] / X.shape[1]\n",
    "                    \n",
    "                # now all the activations in the NN are calculated and stored\n",
    "                \n",
    "                \n",
    "                # now calculate the cost at this iteration\n",
    "                if cost_function == 'binary_cross_entropy':\n",
    "                    cost = - np.sum((Y * np.log(self.A[len(self.layers) - 1]) + (1-Y) * np.log(1 - self.A[len(self.layers) - 1]))) /  X.shape[1] \n",
    "                elif cost_function == 'least_square':\n",
    "                     cost = np.sum((self.A[len(self.layers) - 1] - Y)**2) / X.shape[1]\n",
    "                else:\n",
    "                    raise Exception(\"not implemented yet\")\n",
    "                \n",
    "                #inlcude the regularization term in cost function\n",
    "                cost += cost_reg_term\n",
    "                self.cost_history[-1].append(cost)\n",
    "                # done : cost calc and stroing\n",
    "\n",
    "               \n",
    "\n",
    "                ##### BACK PROP #####\n",
    "                # The order l = L,L-1,L-2,...,3,2,1 (and no 0)\n",
    "                for i in range(len(self.layers) -1 , 0 , -1): # i=0 is excluded  # note :- L is at  len(self.layers) -1 index\n",
    "\n",
    "                    # dZ[L] depends on the choice of cost function\n",
    "                    if i == len(self.layers) -1:\n",
    "                                if cost_function == 'binary_cross_entropy':\n",
    "                                    dL_dA = -(Y/self.A[i]) +((1-Y)/(1-self.A[i]))   \n",
    "                                        \n",
    "#                                        \n",
    "                                elif cost_function == 'least_square':\n",
    "                                    dL_dA = 2 * (self.A[i] - Y)\n",
    "                                    \n",
    "                                   \n",
    "                                    \n",
    "                                # finally we need the dZ (independent from cost function)\n",
    "                                self.dZ[i] = dL_dA * self.activation_derivative_def[i](self.Z[i])\n",
    "\n",
    "\n",
    "\n",
    "                    else: # not the last layer\n",
    "                        self.dZ[i] =  (self.W[i+1].T @ self.dZ[i+1]) * self.activation_derivative_def[i](self.Z[i])\n",
    "                    \n",
    "                    \n",
    "                    # calculate gradients\n",
    "                    m = X.shape[1]\n",
    "                    self.dW[i] = self.dZ[i]@self.A[i-1].T / m\n",
    "                    self.dB[i] = np.sum(self.dZ[i],axis=1,keepdims=1) / m\n",
    "\n",
    "                    # gradient decent\n",
    "                    # the regularization has to be included here\n",
    "                    reg_lambda =  self.layers[i]['regularization_strength']\n",
    "                    self.W[i] = (1 - reg_lambda*learning_rate/m) * self.W[i] - learning_rate * self.dW[i]\n",
    "                    self.B[i] = self.B[i] - learning_rate * self.dB[i]\n",
    "\n",
    "                \n",
    "        time_now = time.time()\n",
    "        total_time = get_nice_time_dura_str(time_now - start_time)\n",
    "        print(f\"Training ended : n_iters: {n_iters} with learning_rate : {learning_rate}. Time taken : {total_time}\")\n",
    "                \n",
    "            \n",
    "        \n",
    "                      \n",
    "\n",
    "    \n",
    "    \n",
    "###### HELPER FUNCTIONS FOR DNN CLASS ###########    \n",
    "    \n",
    "# define activation functions globally\n",
    "def sigmoid(t):\n",
    "        return 1/ ( 1 + np.exp(-t) )\n",
    "      \n",
    "def relu(t): # from chat gpt : this is safe for any dimension array t\n",
    "    return np.maximum(0,t)\n",
    "\n",
    "def relu_deri(t):\n",
    "    return np.where(t>=0,1.,0.)\n",
    "\n",
    "def sigmoid_deri(t):\n",
    "    return (1-sigmoid(t)) * sigmoid(t)\n",
    "\n",
    "def linear(t):\n",
    "    return t\n",
    "\n",
    "def linear_deri(t):\n",
    "    return 1\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "def tanh_deri(Z):\n",
    "    return 1 - np.tanh(Z)**2\n",
    "                \n",
    "ACTIVATION_FUNC = {\"relu\":relu,\"sigmoid\":sigmoid,\"linear\":linear,\"tanh\":tanh}\n",
    "ACTIVATION_FUNC_DERI = {\"relu\":relu_deri,\"sigmoid\":sigmoid_deri,\"linear\":linear_deri,\"tanh\":tanh_deri}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# helper functions\n",
    "def get_nice_time_dura_str(time_in_secs):\n",
    "    time_in_secs = round(time_in_secs,2)\n",
    "    if time_in_secs >= 60:\n",
    "        n_mins = int(time_in_secs//60)\n",
    "        n_secs = round(time_in_secs%60,2)\n",
    "        return f\"{n_mins} min {n_secs} secs\"\n",
    "    return f\"{time_in_secs} secs\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57232f24",
   "metadata": {},
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b98ed1c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T17:54:06.042084Z",
     "start_time": "2023-08-25T17:54:05.899002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#testing\n",
    "\n",
    "def get_spiral_distribution(n_points=100,noise_stats=(0,0),revolutions=1,radial_offset=0):\n",
    "    '''\n",
    "    Generate a set of (x1,x2) domain for spiral with mentioned noise stats : (mean,sigma)\n",
    "    '''\n",
    "    theta = np.random.rand(n_points) * 2 * np.pi * revolutions\n",
    "    radius = np.sqrt(theta) + radial_offset\n",
    "    mean,sigma = noise_stats\n",
    "    noise_for_radius = np.random.randn(n_points) * sigma + mean\n",
    "    radius += noise_for_radius\n",
    "    x1 = radius * np.cos(theta)\n",
    "    x2 = radius * np.sin(theta)\n",
    "    X = np.concatenate((x1.reshape(1,-1),x2.reshape(1,-1)))\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "spiral_X1 = get_spiral_distribution(revolutions=1.6,noise_stats=(0,0.05))\n",
    "spiral_X2 = get_spiral_distribution(revolutions=1.9,radial_offset=0.8,noise_stats=(0,0.05))\n",
    "\n",
    "\n",
    "#plot and see\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.scatter(spiral_X1[0,:],spiral_X1[1,:],color='green')\n",
    "ax.scatter(spiral_X2[0,:],spiral_X2[1,:],color='red')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7015f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T17:54:11.298626Z",
     "start_time": "2023-08-25T17:54:11.278560Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the data set <-- combine positive and negative examples to a single array\n",
    "X_spiral_train = np.hstack((spiral_X1,spiral_X2))\n",
    "Y_spiral_train = np.hstack( ( np.ones((1,spiral_X1.shape[1])),\n",
    "                              np.zeros((1,spiral_X2.shape[1])) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "aa71bafa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T07:17:41.336091Z",
     "start_time": "2023-08-24T07:17:10.668446Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76, 0.76, 0.745, 0.73, 0.76, 0.75, 0.805, 0.8, 0.7, 0.78, 0.765, 0.735, 0.715, 0.745]\n"
     ]
    }
   ],
   "source": [
    "# try multiple and see the results\n",
    "tasks = [delayed(total_test)(),delayed(total_test)()]*7\n",
    "\n",
    "print(Parallel(n_jobs=7)(tasks))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03757a3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T17:53:25.562164Z",
     "start_time": "2023-08-25T17:53:21.688839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 8200 ---> ETA : 3 min 33.5 secs  \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15316/2602144023.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mmy_spiral_fa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m my_spiral_fa.batch_fit(X_spiral_train,\n\u001b[0m\u001b[0;32m     30\u001b[0m                        \u001b[0mY_spiral_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                        \u001b[0mcost_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'least_square'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'binary_cross_entropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15316/1821123373.py\u001b[0m in \u001b[0;36mbatch_fit\u001b[1;34m(self, X, Y, cost_function, n_iters, learning_rate)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# not the last layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_derivative_def\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_spiral_fa = DNN(layers=[\n",
    "\n",
    "    {\n",
    "        \"type\":\"input\",\n",
    "        \"units\":2\n",
    "    },\n",
    "\n",
    "\n",
    "    {\n",
    "        \"type\":\"hidden\",\n",
    "        \"units\":32,\n",
    "        \"activation_function\":\"tanh\",\n",
    "        \"regularization_strength\":0.0,\n",
    "        \"dropout_keep_prob\":1.0\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"type\":\"output\",\n",
    "        \"units\":1,\n",
    "        \"activation_function\":\"sigmoid\",\n",
    "        \"regularization_strength\":0.0,\n",
    "        \"dropout_keep_prob\":1.0\n",
    "    }\n",
    "\n",
    "])\n",
    "\n",
    "my_spiral_fa.build()\n",
    "\n",
    "my_spiral_fa.batch_fit(X_spiral_train,\n",
    "                       Y_spiral_train,\n",
    "                       cost_function=('least_square','binary_cross_entropy')[1],\n",
    "                       n_iters=5_00_000,\n",
    "                       learning_rate=1e-2)\n",
    "\n",
    "\n",
    "my_spiral_fa.show_cost_history()\n",
    "\n",
    "\n",
    "\n",
    "#evaluate\n",
    "predictions = np.int32(my_spiral_fa.predict(X_spiral_train) > 0.5)\n",
    "accuracy = (predictions == Y_spiral_train).sum() / Y_spiral_train.shape[1]\n",
    "\n",
    "display(HTML(f\"<br/><font color='red' size='5'><b>Accuracy : {accuracy}</b></font>\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f4c4ce",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd7199",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T17:53:25.562164Z",
     "start_time": "2023-08-25T17:53:25.562164Z"
    }
   },
   "outputs": [],
   "source": [
    "# from predictions clasify the training set in to positive and negatives\n",
    "pred_positives = X_spiral_train[:,predictions.reshape(-1)==1]\n",
    "pred_negatives = X_spiral_train[:,predictions.reshape(-1)!=1]\n",
    "\n",
    "#filter the wrongly identified ones\n",
    "wrong_ones = X_spiral_train[:,predictions.reshape(-1)!=Y_spiral_train.reshape(-1)]\n",
    "\n",
    "#plot and see\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,8))\n",
    "\n",
    "ax[0].scatter(spiral_X1[0,:],spiral_X1[1,:],color='green',label=\"Positives\")\n",
    "ax[0].scatter(spiral_X2[0,:],spiral_X2[1,:],color='red',label=\"Negatives\")\n",
    "ax[0].set_title(\"The training data set\")\n",
    "\n",
    "ax[1].scatter(pred_positives[0,:],pred_positives[1,:],color='green',label=\"Positives\")\n",
    "ax[1].scatter(pred_negatives[0,:],pred_negatives[1,:],color='red',label=\"Negatives\")\n",
    "ax[1].set_title(\"Predicted after training\")\n",
    "\n",
    "ax[1].scatter(wrong_ones[0,:],wrong_ones[1,:],facecolors='none', edgecolors='brown', s=300,label=\"wrongly predicted\")\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
