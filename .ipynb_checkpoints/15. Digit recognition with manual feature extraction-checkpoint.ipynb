{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33384bb0",
   "metadata": {},
   "source": [
    "\n",
    "# Digit Recognition - MNIST dataset - Manual feature extraction\n",
    "\n",
    "\\subsection{Methodology}\n",
    "\n",
    "### Image feature extraction\n",
    "Gray scale image $ I_{28 \\times 28} $ is convoled with  $  N_k $ number of 3x3 kernels  ( $ K_{3 \\times 3}^{(i)}  $ for $ i= 1,2,..., N_k$ ) then applied Gaussian-blur ($ kernel-size = 5 \\times 5 $ with $\\sigma=5 $) and thresholded and  finally max-pooled with  stride = $ 4  $ window-size = $ 4 \\times 4 $ and the result is stored in tensor $ R_{7 \\times 7 \\times N_k} $ . \\\n",
    " $ R_{7 \\times 7 \\times N_k} $ contains all the features per single image $ I_{28 \\times 28} $.\\\n",
    " \\\n",
    " Now, to feed the NN, input need to be a vector. So store the features $ \\vec{x^{(i)}} = \\mathrm{flatten} ( R_{7 \\times 7 \\times N_k} ^ {(i)} ) $\n",
    " \n",
    " ### Train Neural network\n",
    " Using a simple one layer network with $ n ^{[0]} = 7 \\times 7 \\times N_{k} $ input nodes and $ n^{[1]} = 10 $ output nodes, using $ \\mathrm{sigmoid}$ activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "082e1e94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T18:19:17.592217Z",
     "start_time": "2023-11-30T18:19:16.103877Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import HTML,Latex\n",
    "import os \n",
    "from joblib import Parallel,delayed\n",
    "import json\n",
    "import cv2 as cv\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171d3f8",
   "metadata": {},
   "source": [
    "### The Deep Neural Network is self implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "71d261ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:00:17.272747Z",
     "start_time": "2023-12-01T05:00:17.164334Z"
    }
   },
   "outputs": [],
   "source": [
    "PARAM_FOLDER_NAME = 'parameters_as_json/'\n",
    "class DNN:\n",
    "    def __init__(self,layers):\n",
    "        # layers :  list of dicts containing later information.\n",
    "        assert type(layers) in  (list,tuple)\n",
    "        assert len(layers) >= 2, \"At least an input and an output layer should be there\"\n",
    "        \n",
    "        # validate each layer\n",
    "        for i,layer in enumerate(layers):\n",
    "            if i==0:\n",
    "                assert layer[\"type\"] == \"input\", \"first layer should be input\"\n",
    "            elif i==len(layers)-1:\n",
    "                assert layer[\"type\"] == \"output\", \"last layer should be output\"\n",
    "            else:\n",
    "                assert layer[\"type\"] == \"hidden\", \"middle layers should be hidden\"\n",
    "                assert layer[\"activation_function\"] in ACTIVATION_FUNC\n",
    "                \n",
    "            assert type(layer[\"units\"]) == int\n",
    "            \n",
    "            #if the reqularization term is not mentioned --> assign 0.0\n",
    "            if 'regularization_strength' not in layer:\n",
    "                layer['regularization_strength'] = 0.0\n",
    "            # if the keep prob is not mentioned --> assign 1.0\n",
    "            if 'dropout_keep_prob' not in layer:\n",
    "                layer['dropout_keep_prob'] = 1.0\n",
    "            \n",
    "            #validate the regularization and keep prob values\n",
    "            assert type(layer['regularization_strength']) == type(layer['dropout_keep_prob']) == float,\\\n",
    "                    \"regularization_strength and dropout_keep_prob should be float\"\n",
    "                \n",
    "        #done: validation\n",
    "        \n",
    "        #save these info\n",
    "        self.layers = layers\n",
    "        \n",
    "        #keep state variables\n",
    "        self.n_ever_trained = 0\n",
    "        self.total_iterations = 0;\n",
    "        self.parameters_as_json_files = []\n",
    "        \n",
    "        #costs need to be saved separately per each training\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def absorb_parameters(self,W,B):\n",
    "        self.W=W\n",
    "        self.B=B\n",
    "        \n",
    "    def load_history_state(self,history_state):\n",
    "        history_state = str(history_state)\n",
    "        # only need the random float --> format if underscore and some other things in the given name\n",
    "        if '_' in history_state:\n",
    "            history_state = history_state.split('_')[0]\n",
    "        if not history_state.startswith('0.'):\n",
    "            history_state = '0.' + history_state\n",
    "        #open the jason file and load the parameters\n",
    "        with open(PARAM_FOLDER_NAME + history_state + '_params.json','r') as file:\n",
    "            all_params =  json.load(file)\n",
    "            all_params['W'] = [np.array(w) if w is not None else None for w in all_params['W']]\n",
    "            all_params['B'] = [np.array(b) if b is not None else None for b in all_params['B']]\n",
    "            self.absorb_parameters(all_params['W'],all_params['B'])\n",
    "            print('\\033[31m Reset the parameters according to history state : ' + history_state,'\\033[30m')\n",
    "            self.parameters_as_json_files.append(history_state + '_params.json')\n",
    "        \n",
    "        \n",
    "    def build(self,show=0,weight_init_noise_amplitude=0.01):\n",
    "        # the parameters will be initialized with required dimentions\n",
    "        \n",
    "        self.W = [None] # this array will keep weight matrix  per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.B = [None] # this array will keep bias vector  per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.activation_def = [None] # the pointer to respecting activation function's definition per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.activation_derivative_def = [None] # the pointer to respecting activation function's derivative's  \n",
    "                                                                #definition per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        \n",
    "        self.A = [] # keep all intermidiate activation matrix per each layer (l=1,2,..,L)\n",
    "        self.Z = [] # keep all intermidiate pre-activation matrix per each layer (l=1,2,..,L)\n",
    "        \n",
    "        #initializing\n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            # anything for l=0,1,2,..,L ? \n",
    "            # NO\n",
    "             \n",
    "            \n",
    "            if i: #only start from layer 1,2,..,L\n",
    "                weight_matrix = np.float32(np.random.randn(self.layers[i][\"units\"],self.layers[i-1][\"units\"]) * weight_init_noise_amplitude)\n",
    "                bias_vector = np.zeros((self.layers[i][\"units\"],1),dtype=np.float32)\n",
    "                self.W.append(weight_matrix)\n",
    "                self.B.append(bias_vector)\n",
    "                   \n",
    "               \n",
    "                self.activation_def.append(ACTIVATION_FUNC[self.layers[i][\"activation_function\"]])\n",
    "                self.activation_derivative_def.append(ACTIVATION_FUNC_DERI[self.layers[i][\"activation_function\"]])\n",
    "           \n",
    "            \n",
    "        if show: \n",
    "            print(self.W) \n",
    "            print(self.B)\n",
    "    \n",
    "    def show_image(self,scale=10):\n",
    "        def scaled(x):return scale*x\n",
    "        \n",
    "        layers = self.layers\n",
    "        n_layers = len(layers)\n",
    "        n_neurons = []\n",
    "        for layer in layers:\n",
    "            n_neurons.append(layer['units'])\n",
    "            \n",
    "        # image height and the width need to be estimated\n",
    "        WIDTH_PER_LAYER = scaled(50)\n",
    "        RADIUS_OF_NEURON = scaled(5)\n",
    "        NEURON_SPACING = scaled(2)\n",
    "        width = WIDTH_PER_LAYER * (n_layers-1) + WIDTH_PER_LAYER # added WIDTH_PER_LAYER for padding\n",
    "        height = (2*RADIUS_OF_NEURON + NEURON_SPACING) * max(n_neurons)  +  NEURON_SPACING\n",
    "        \n",
    "        #initialize the canvas (white background)\n",
    "        image = np.zeros((height, width, 3), dtype=np.uint8) + 255\n",
    "        \n",
    "        #drawing the neurons and keeping records on the neuron origin coords\n",
    "        neuron_coords = []\n",
    "        x_pointer = WIDTH_PER_LAYER // 2\n",
    "        movement_per_neuron = 2*RADIUS_OF_NEURON + NEURON_SPACING\n",
    "        for i,layer in enumerate(layers):\n",
    "            y_pointer = NEURON_SPACING + RADIUS_OF_NEURON\n",
    "            # need to vertically center the neurons in hidden layers with less than maximum \n",
    "            extra_space_avail = movement_per_neuron * (max(n_neurons) - n_neurons[i])\n",
    "            y_padding = extra_space_avail // 2\n",
    "            y_pointer+=y_padding\n",
    "            \n",
    "            this_layer_neuron_coords = []\n",
    "            for ni in range(n_neurons[i]):\n",
    "                this_layer_neuron_coords.append((x_pointer,y_pointer))\n",
    "                cv.circle(image,(x_pointer,y_pointer),RADIUS_OF_NEURON,(0,0,0),-1) #r3ki3g skipped scale for thickness\n",
    "                y_pointer+= movement_per_neuron\n",
    "            x_pointer+=WIDTH_PER_LAYER\n",
    "            neuron_coords.append(this_layer_neuron_coords)\n",
    "            \n",
    "        # drawing the lines to represent the connections between neurons\n",
    "        for i in range(1,n_layers):\n",
    "            this_layer_index = i-1\n",
    "            next_layer_index = i\n",
    "            \n",
    "            for start_point in neuron_coords[this_layer_index]:\n",
    "                for end_point in neuron_coords[next_layer_index]:\n",
    "                    cv.line(image,start_point,end_point,(0,0,0),2)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        fig,ax= plt.subplots(1)\n",
    "        ax.imshow(image)\n",
    "        plt.show()\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "    def show_cost_history(self,log=0,grid=1):\n",
    "        PLOT_COLORS = ['blue','red','green','orange']\n",
    "        fig,ax = plt.subplots(1)\n",
    "        pointer = 0\n",
    "        for i in range(len(self.cost_history)):\n",
    "            if not log:\n",
    "                ax.plot(list(range(pointer,pointer:=pointer + len(self.cost_history[i]))),\n",
    "                    self.cost_history[i],\n",
    "                    color = PLOT_COLORS[i%len(PLOT_COLORS)])\n",
    "            else: # need to plot the log of costs\n",
    "                ax.plot(list(range(pointer,pointer:=pointer + len(self.cost_history[i]))),\n",
    "                    np.log(self.cost_history[i]),\n",
    "                    color = PLOT_COLORS[i%len(PLOT_COLORS)])\n",
    "                \n",
    "        #generate a title for the plot\n",
    "        plot_title = f'Cost vs. iterration '\n",
    "        if log:\n",
    "            plot_title += ':: LOG'\n",
    "        ax.set_title(plot_title)\n",
    "        if grid:\n",
    "            ax.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        X = np.array(X,dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        # these are some place holders to keep the arrays in required size\n",
    "        self.A = [None for i in range(len(self.layers))]\n",
    "        self.Z = [None for i in range(len(self.layers))]\n",
    "        self.A[0] = X # the input matrix\n",
    "        \n",
    "       \n",
    "        for i in range(1,len(self.layers)):\n",
    "                    self.Z[i] = self.W[i]@self.A[i-1]+self.B[i]\n",
    "                    self.A[i] = self.activation_def[i](self.Z[i])\n",
    "        # now all the activations in the NN are calculated and stored\n",
    "        \n",
    "        return self.A[len(self.layers) - 1]\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "    def __gen_progress_msg_pieces(self,n_iters,iteration,start_time,train_accuracy, X_train,Y_train,test_accuracy,X_test,Y_test):\n",
    "            time_now = time.time()\n",
    "            time_remaining = get_nice_time_dura_str ((n_iters - iteration) * (time_now - start_time) / iteration)\n",
    "\n",
    "            # accuracy caculations                        \n",
    "            if  test_accuracy is not None:\n",
    "                test_accuracy_phrase = ' test acc : {}'.format(test_accuracy(self,X_test,Y_test))\n",
    "            if  train_accuracy is not None:\n",
    "                train_accuracy_phrase = ' train acc : {}'.format(train_accuracy(self,X_train,Y_train))     \n",
    "            return time_remaining,train_accuracy_phrase,test_accuracy_phrase\n",
    "                        \n",
    "        \n",
    "        \n",
    "    def batch_fit(self,X,Y,cost_function='least_square',\n",
    "                  n_iters=1_000,learning_rate=1e-3,\n",
    "                  ADAM=True,beta_1=0.9,beta_2=0.99,adam_epsilon=1e-2,\n",
    "                 test_accuracy=None,\n",
    "                 train_accuracy=None,\n",
    "                 X_test=None,\n",
    "                 Y_test=None,\n",
    "                 inline=False):\n",
    "        \n",
    "        assert cost_function in ('least_square','binary_cross_entropy')\n",
    "        Y = np.float32(Y)\n",
    "        X = np.float32(X)\n",
    "      \n",
    "        __printlnend = '\\r' if inline else '\\n'\n",
    "\n",
    "        \n",
    "        if not self.n_ever_trained:\n",
    "            # these are some place holders to keep the arrays in required size\n",
    "            self.A = [None for i in range(len(self.layers))]\n",
    "            self.Z = [None for i in range(len(self.layers))]\n",
    "\n",
    "            self.dZ = [None for i in range(len(self.layers))]\n",
    "            self.dA = [None for i in range(len(self.layers))]\n",
    "            self.dW = [None for i in range(len(self.layers))]\n",
    "            self.dB = [None for i in range(len(self.layers))]\n",
    "            \n",
    "            #placeholders for dropout vectors per each layer\n",
    "            self.dropout_vectors = [None] * (len(self.layers)-1) #for the output layer no need of dropout vector\n",
    "            \n",
    "             # For ADAM, initialize the following\n",
    "            if ADAM:\n",
    "                self.__initialize_RMSprop_with_zeros()\n",
    "                self.__initialize_momentum_with_zeros()\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print(\"Training again..fine tuning of parameters continued from where left at last time...\")\n",
    "\n",
    "        #update the state variable\n",
    "        self.n_ever_trained += 1\n",
    "            \n",
    "        self.A[0] = X # set the input matrix \n",
    "             \n",
    "       \n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.cost_history.append([])\n",
    "\n",
    "        for iteration in range(n_iters): \n",
    "            \n",
    "                # progress message : every 100 iteration except for the first one\n",
    "                if iteration and not iteration%10:\n",
    "                        (time_remaining,\n",
    "                        train_accuracy_phrase,\n",
    "                        test_accuracy_phrase) = self.__gen_progress_msg_pieces(n_iters,iteration,start_time,train_accuracy,\n",
    "                                                                       X_train,Y_train,test_accuracy,X_test,Y_test)\n",
    "                        \n",
    "                        \n",
    "                        print(f\"iteration : {iteration}/{n_iters} ---> ETA : {time_remaining} \" + \\\n",
    "                              train_accuracy_phrase +\\\n",
    "                              test_accuracy_phrase,\n",
    "                              \n",
    "                              end=__printlnend)\n",
    "                # end : progress message\n",
    "            \n",
    "            \n",
    "            \n",
    "                ##### FWD PASS #####\n",
    "                #  r3ki3g : assumes the X in pre-scaled / normalized\n",
    "                \n",
    "                # apply droput settings for the input layer only\n",
    "                should_not_drop = np.random.rand(*self.A[0].shape) < self.layers[0][\"dropout_keep_prob\"]\n",
    "                self.dropout_vectors[0] = should_not_drop\n",
    "                self.A[0] *= should_not_drop /  self.layers[0][\"dropout_keep_prob\"]\n",
    "\n",
    "                # loop though each layer and calculate the pre-activations(Z) and activations(A)\n",
    "                cost_reg_term = 0\n",
    "                for i in range(1,len(self.layers)):\n",
    "                    self.Z[i] = self.W[i]@self.A[i-1]+self.B[i]\n",
    "                    self.A[i] = self.activation_def[i](self.Z[i])\n",
    "                    \n",
    "                    # consider the drop-out settings (except in ouput layer)\n",
    "                    if i != len(self.layers)-1:\n",
    "                        should_not_drop = np.random.rand(*self.A[i].shape) <= self.layers[i][\"dropout_keep_prob\"]\n",
    "                        self.dropout_vectors[i] = should_not_drop\n",
    "                        self.A[i] *= should_not_drop /  self.layers[i][\"dropout_keep_prob\"]\n",
    "                    \n",
    "                    #iclude the regularization term in cost function (temp:cost_reg_term )\n",
    "                    cost_reg_term += np.sum(self.W[i]**2) * self.layers[i][\"regularization_strength\"] / X.shape[1]\n",
    "                    \n",
    "                # now all the activations in the NN are calculated and stored\n",
    "                \n",
    "                \n",
    "                # now calculate the cost at this iteration\n",
    "                if cost_function == 'binary_cross_entropy':\n",
    "                    # 1e-8 added inside log functions to avoid occuring log(0)\n",
    "                    cost = - np.sum((Y * np.log(1e-8 + self.A[len(self.layers) - 1]) + (1.-Y) * np.log(1e-8 + 1. - self.A[len(self.layers) - 1]))) /  X.shape[1] \n",
    "                elif cost_function == 'least_square':\n",
    "                     cost = np.sum((self.A[len(self.layers) - 1] - Y)**2) / X.shape[1]\n",
    "                else:\n",
    "                    raise Exception(\"not implemented yet\")\n",
    "                \n",
    "                #inlcude the regularization term in cost function\n",
    "                cost += cost_reg_term\n",
    "                self.cost_history[-1].append(cost)\n",
    "                # done : cost calc and stroing\n",
    "\n",
    "               \n",
    "\n",
    "                ##### BACK PROP #####\n",
    "                # The order l = L,L-1,L-2,...,3,2,1 (and no 0)\n",
    "                for i in range(len(self.layers) -1 , 0 , -1): # i=0 is excluded  # note :- L is at  len(self.layers) -1 index\n",
    "\n",
    "                    # the output layer pre-activation AKA dZ[L] depends on the choice of cost function\n",
    "                    if i == len(self.layers) -1:\n",
    "                                if cost_function == 'binary_cross_entropy':\n",
    "                                    dL_dA = -(Y/self.A[i]) +((1-Y)/(1-self.A[i]))   \n",
    "                                        \n",
    "#                                        \n",
    "                                elif cost_function == 'least_square':\n",
    "                                    dL_dA = 2 * (self.A[i] - Y)\n",
    "                                    \n",
    "                                   \n",
    "                                    \n",
    "                                # finally we need the dZ (independent from cost function)\n",
    "                                self.dZ[i] = dL_dA * self.activation_derivative_def[i](self.Z[i])\n",
    "\n",
    "\n",
    "\n",
    "                    else: # not the last layer\n",
    "                        self.dZ[i] =  (self.W[i+1].T @ self.dZ[i+1]) * self.activation_derivative_def[i](self.Z[i])\n",
    "                        # dropout affects backpropagation as well\n",
    "                        self.dZ[i] *= self.dropout_vectors[i] / self.layers[i][\"dropout_keep_prob\"]\n",
    "                    \n",
    "                    \n",
    "                    # calculate gradients\n",
    "                    m = X.shape[1]\n",
    "                    self.dW[i] = self.dZ[i]@self.A[i-1].T / m\n",
    "                    self.dB[i] = np.sum(self.dZ[i],axis=1,keepdims=1) / m\n",
    "\n",
    "                    # gradient decent\n",
    "                    # ADAM optimization has to be considered \n",
    "                    \n",
    "                    if 'keep a record of the previous state':\n",
    "                        vdWh = self.vdW[i]\n",
    "                        vdBh = self.vdB[i]\n",
    "                        sdWh = self.sdW[i]\n",
    "                        sdBh = self.sdB[i]\n",
    "                    \n",
    "                    \n",
    "                    self.vdW[i] = beta_1 * self.vdW[i]  + (1. - beta_1) * self.dW[i]\n",
    "                    self.vdB[i] = beta_1 * self.vdB[i]  + (1. - beta_1) * self.dB[i]\n",
    "                    self.sdW[i] = beta_2 * self.sdW[i]  + (1. - beta_2) * self.dW[i]**2\n",
    "                    self.sdB[i] = beta_2 * self.sdB[i]  + (1. - beta_2) * self.dB[i]**2\n",
    "                  \n",
    "\n",
    "                    vdW = self.vdW[i] / (1. - beta_1**(iteration+1))\n",
    "                    vdB = self.vdB[i] / (1. - beta_1**(iteration+1))\n",
    "                    sdW = self.sdW[i] / (1. - beta_2**(iteration+1))\n",
    "                    sdB = self.sdB[i] / (1. - beta_2**(iteration+1))\n",
    "                    \n",
    "                    \n",
    "                    if not 'print the error caused states':\n",
    "                        \n",
    "                        if (np.any(np.isnan(vdW))):\n",
    "                            print(\"nan detedted\",iteration)\n",
    "                            print(\"vdWh\",vdWh)\n",
    "                            print(\"end\")\n",
    "                        if (np.any(np.isnan(vdB))):\n",
    "                            print(\"nan detedted\",iteration)\n",
    "                            print(\"vdBh\",vdBh)\n",
    "                            print(\"end\")\n",
    "                        if (np.any(np.isnan(sdW))):\n",
    "                            print(\"nan detedted\",iteration)\n",
    "                            print(\"sdWh\",sdWh)\n",
    "                            print(\"end\")\n",
    "                        if (np.any(np.isnan(sdB))):\n",
    "                            print(\"nan detedted\",iteration)\n",
    "                            print(\"sdBh\",sdBh)\n",
    "                            print(\"end\")\n",
    "\n",
    "\n",
    "    #                     if iteration<4 or iteration>n_iters-2:\n",
    "    #                         print(\"vdW\",vdW)\n",
    "\n",
    "    #                         print(\"vdB\",vdB)\n",
    "\n",
    "    #                         print(\"sdW\",sdW)\n",
    "    #                         print(\"sdB\",sdB)\n",
    "\n",
    "                    \n",
    "                  \n",
    "                    # the regularization has to be included here\n",
    "                    reg_lambda =  self.layers[i]['regularization_strength']\n",
    "                    \n",
    "#                     self.W[i] = (1 - reg_lambda*learning_rate/m) * self.W[i] \\\n",
    "#                                 - (learning_rate / ( np.sqrt(np.abs(self.sdW[i])) + adam_epsilon) ) * self.vdW[i]\n",
    "#                     self.B[i] = self.B[i] \\\n",
    "#                                 - (learning_rate / (np.sqrt(np.abs(self.sdB[i])) + adam_epsilon) ) * self.vdB[i]\n",
    "                    \n",
    "    \n",
    "                    self.W[i] = (1 - reg_lambda*learning_rate/m) * self.W[i] \\\n",
    "                                - (learning_rate / ( np.sqrt(sdW + 1e-8) + adam_epsilon) ) * vdW\n",
    "                    self.B[i] = self.B[i] \\\n",
    "                                - (learning_rate / ( np.sqrt(sdB + 1e-8) + adam_epsilon) ) * vdB\n",
    "\n",
    "        #update the state variables\n",
    "        self.total_iterations += n_iters\n",
    "        \n",
    "        time_now = time.time()\n",
    "        total_time = get_nice_time_dura_str(time_now - start_time)\n",
    "        _,train_accuracy_phrase,test_accuracy_phrase = self.__gen_progress_msg_pieces(n_iters,iteration,start_time,\n",
    "                                                                                      train_accuracy,X_train,Y_train,\n",
    "                                                                                      test_accuracy,X_test,Y_test)\n",
    "        \n",
    "        print(f\"Training ({self.n_ever_trained}) ended  : n_iters: {n_iters} with learning_rate : {learning_rate}. Time taken : {total_time}\")\n",
    "        print(f'Total summary :: iterrations : {self.total_iterations} {train_accuracy_phrase}{test_accuracy_phrase}')\n",
    "        \n",
    "        #saving the weights in to a file\n",
    "        self.__save_current_learnt_params()\n",
    "        \n",
    "    def history(self):\n",
    "        print('Follwoing history sates are available:')\n",
    "        print(self.parameters_as_json_files)\n",
    "        print(\"Use DNN.load_history_state(history_state) to reset parameters to any state.\")\n",
    "        print(\"Use DNN.back() to ignore last training and return to the previous state.\")\n",
    "        \n",
    "    def back(self):\n",
    "        undo_last__filename = self.parameters_as_json_files[-2]\n",
    "        self.load_history_state(undo_last__filename)\n",
    "        self.parameters_as_json_files.append(undo_last__filename)\n",
    "        print(\"Last training was ignored and returned to the previous state: \" + undo_last__filename)\n",
    "    \n",
    "    def layer(self,name):\n",
    "        selected_layer = None\n",
    "        for layer in self.layers:\n",
    "            if 'name' in layer and layer['name'] == name:\n",
    "                if selected_layer == None:\n",
    "                    selected_layer = layer\n",
    "                else:\n",
    "                    raise Exception(\"Found multiple layers with name : \" +  name)\n",
    "        if selected_layer == None:        \n",
    "            raise Exception(\"No layer with the name : \" + name)\n",
    "        return selected_layer\n",
    "        \n",
    "                \n",
    "            \n",
    "    def __initialize_momentum_with_zeros(self):\n",
    "        self.vdW = [np.zeros(layer_w.shape,dtype=np.float32) if layer_w is not None else None for layer_w in self.W]\n",
    "        self.vdB = [np.zeros(layer_b.shape,dtype=np.float32) if layer_b is not None else None for layer_b in self.B]\n",
    "    def __initialize_RMSprop_with_zeros(self):\n",
    "        self.sdW = [np.zeros(layer_w.shape,dtype=np.float32) if layer_w is not None else None for layer_w in self.W]\n",
    "        self.sdB = [np.zeros(layer_b.shape,dtype=np.float32) if layer_b is not None else None for layer_b in self.B]\n",
    "\n",
    "    def __save_current_learnt_params(self):\n",
    "        #generate a random name for the training-history-point\n",
    "        this_status_name = str(np.random.rand()) + '_params.json'\n",
    "        #make a 'parameters as json' folder if it does not exist already\n",
    "        \n",
    "        if not os.path.isdir(PARAM_FOLDER_NAME):os.mkdir(PARAM_FOLDER_NAME)\n",
    "        with open(PARAM_FOLDER_NAME + this_status_name,'w') as file:\n",
    "            weights = [w.tolist() if not w is None else None for w in self.W]\n",
    "            biases = [b.tolist() if not b is None else None for b in self.B]\n",
    "            all_params = {\"W\":weights,\"B\":biases}\n",
    "            json.dump(all_params,file)\n",
    "            \n",
    "        self.parameters_as_json_files.append(this_status_name)\n",
    "        print(\"saved params to: \" + this_status_name)\n",
    "        \n",
    "                      \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ec6af771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T04:54:53.080319Z",
     "start_time": "2023-12-01T04:54:53.060631Z"
    }
   },
   "outputs": [],
   "source": [
    "    \n",
    "###### HELPER FUNCTIONS FOR DNN CLASS ###########    \n",
    "    \n",
    "# define activation functions globally\n",
    "def sigmoid(t):\n",
    "        return 1/ ( 1 + np.exp(-t) )\n",
    "      \n",
    "def relu(t): # from chat gpt : this is safe for any dimension array t\n",
    "    return np.maximum(0,t)\n",
    "\n",
    "def relu_deri(t):\n",
    "    return np.where(t>=0,1.,0.)\n",
    "\n",
    "def sigmoid_deri(t):\n",
    "    return (1-sigmoid(t)) * sigmoid(t)\n",
    "\n",
    "def linear(t):\n",
    "    return t\n",
    "\n",
    "def linear_deri(t):\n",
    "    return 1\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "def tanh_deri(Z):\n",
    "    return 1 - np.tanh(Z)**2\n",
    "                \n",
    "ACTIVATION_FUNC = {\"relu\":relu,\"sigmoid\":sigmoid,\"linear\":linear,\"tanh\":tanh}\n",
    "ACTIVATION_FUNC_DERI = {\"relu\":relu_deri,\"sigmoid\":sigmoid_deri,\"linear\":linear_deri,\"tanh\":tanh_deri}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# helper function for displaying ETA in a nice manner\n",
    "def get_nice_time_dura_str(time_in_secs):\n",
    "    time_in_secs = round(time_in_secs,2)\n",
    "    if time_in_secs >= 60:\n",
    "        n_mins = int(time_in_secs//60)\n",
    "        n_secs = round(time_in_secs%60,2)\n",
    "        return f\"{n_mins} min {n_secs} secs\"\n",
    "    return f\"{time_in_secs} secs\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbb16d4",
   "metadata": {},
   "source": [
    "### Max pooling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0236e2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T18:19:29.239900Z",
     "start_time": "2023-11-30T18:19:29.223978Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def max_pooling(input_array, pool_size=(2, 2)):\n",
    "    \"\"\"\n",
    "    Perform max pooling on a 2D input array.\n",
    "\n",
    "    Parameters:\n",
    "    - input_array: 2D numpy array, input data for pooling\n",
    "    - pool_size: tuple, size of the pooling window (default is (2, 2))\n",
    "\n",
    "    Returns:\n",
    "    - numpy array, result of max pooling\n",
    "    \"\"\"\n",
    "\n",
    "    # Get input array shape\n",
    "    input_shape = input_array.shape\n",
    "\n",
    "    # Get the size of the pooling window\n",
    "    window_size_row, window_size_col = pool_size\n",
    "\n",
    "    # Calculate the output shape\n",
    "    output_shape = (\n",
    "        input_shape[0] // window_size_row,\n",
    "        input_shape[1] // window_size_col,\n",
    "    )\n",
    "\n",
    "    # Initialize the output array with zeros\n",
    "    output_array = np.zeros(output_shape)\n",
    "\n",
    "    # Iterate through each row and column\n",
    "    for i in range(output_shape[0]):\n",
    "        for j in range(output_shape[1]):\n",
    "            # Extract the region to perform max pooling\n",
    "            pool_region = input_array[\n",
    "                i * window_size_row : (i + 1) * window_size_row,\n",
    "                j * window_size_col : (j + 1) * window_size_col,\n",
    "            ]\n",
    "\n",
    "            # Compute the maximum value in the region\n",
    "            output_array[i, j] = np.max(pool_region)\n",
    "\n",
    "    return output_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad539bd",
   "metadata": {},
   "source": [
    "### Generate a matrix with one hot row \n",
    "To encode the output classes, need matrices with only one row (class) being 1 and everywhere else is 0.\\\n",
    "Example:-\\\n",
    "\n",
    "\n",
    "| `0` `0` `0` `0` `0` `0` `0` `0` `0` `0` | \\\n",
    "| `1` `1` `1` `1` `1` `1` `1` `1` `1` `1` | \\\n",
    "| `0` `0` `0` `0` `0` `0` `0` `0` `0` `0` | \\\n",
    "| `0` `0` `0` `0` `0` `0` `0` `0` `0` `0` | \\\n",
    "| `0` `0` `0` `0` `0` `0` `0` `0` `0` `0` | \\\n",
    "| `0` `0` `0` `0` `0` `0` `0` `0` `0` `0` | \\\n",
    "| `0` `0` `0` `0` `0` `0` `0` `0` `0` `0` | \\\n",
    "\\\n",
    "The second class is \"1\" but elsewhere is \"0\" : n_ones_in_r_th_row_in_mXn_zero_matrix(7,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c72cd6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T18:19:31.781827Z",
     "start_time": "2023-11-30T18:19:31.769938Z"
    }
   },
   "outputs": [],
   "source": [
    "def n_ones_in_r_th_row_in_mXn_zero_matrix(m,n,r):\n",
    "    temp = np.zeros((m,n))\n",
    "    temp[r,:] = np.ones((1,n))\n",
    "    return temp\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abedcff0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T18:19:32.996150Z",
     "start_time": "2023-11-30T18:19:32.990655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Just testing the data folder\n",
    "# folder = '''D:/ENTC/PROJECTS/digit-recognition-with-convolution-from-scratch/MNIST Dataset JPG format/MNIST Dataset JPG format/training/0/'''\n",
    "# files = os.listdir(folder)\n",
    "# files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba06435",
   "metadata": {},
   "source": [
    "### Define a set of manual kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a5ca79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T18:19:34.705936Z",
     "start_time": "2023-11-30T18:19:34.272794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_KERNELS_NEEDED = 5\n",
    "\n",
    "kernels = [] \n",
    "\n",
    "\n",
    "#sobel vertical - A must \n",
    "k1sv = np.array([[-1,0,1],\n",
    "                 [-2,0,2],\n",
    "                 [-1,0,1]])\n",
    "#sobel horizontal - A must \n",
    "k2sh = k1sv.T\n",
    "\n",
    "kernels.extend([k1sv,k2sh])\n",
    "\n",
    "\n",
    "\n",
    "# generate random kernels to fill up to NUM_KERNELS_NEEDED\n",
    "\n",
    "for i in range(NUM_KERNELS_NEEDED - 2):\n",
    "    temp = 2*(np.random.rand(3,3) > 0.5) - 1\n",
    "    \n",
    "    kernels.append(temp)\n",
    "    \n",
    "    \n",
    "# show the kernels\n",
    "fig,ax = plt.subplots(1,len(kernels),figsize=(9,4))\n",
    "for i in range(len(kernels)):\n",
    "    ax[i].imshow(kernels[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db8d9e6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T18:19:36.799440Z",
     "start_time": "2023-11-30T18:19:36.791072Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep track of the kernels\n",
      "[[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], [[-1, -2, -1], [0, 0, 0], [1, 2, 1]], [[-1, -1, 1], [-1, -1, -1], [1, -1, 1]], [[-1, -1, 1], [-1, -1, -1], [1, 1, 1]], [[-1, -1, -1], [-1, -1, 1], [-1, 1, -1]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Keep track of the kernels\")\n",
    "print(json.dumps([k.tolist() for k in kernels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc98670",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T12:12:04.041565Z",
     "start_time": "2023-11-30T12:12:03.360118Z"
    }
   },
   "outputs": [],
   "source": [
    "# # see some images in the data set\n",
    "\n",
    "# mytestimgs = [folder + '/' + files[100 + i] for i in range(10)]\n",
    "# mytestimgs = [cv.imread(mytestimgs[i],0) for i in range(len(mytestimgs))]\n",
    "\n",
    "# fig, ax = plt.subplots(1,len(mytestimgs))\n",
    "# for i in range(len(mytestimgs)):\n",
    "#     ax[i].imshow(mytestimgs[i],cmap='gray')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6162c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T18:19:38.996356Z",
     "start_time": "2023-11-30T18:19:38.991862Z"
    }
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-12\n",
    "logepsilon = np.log(epsilon)\n",
    "def get_features(image):\n",
    "    assert image is not None\n",
    "    convres = []\n",
    "    for j in range(len(kernels)):\n",
    "        kernel = kernels[j]\n",
    "        imgres = cv.filter2D(image,-1,kernel)\n",
    "        imgres = cv.GaussianBlur(imgres,(5,5),5)\n",
    "        \n",
    "        # two options\n",
    "        #imgres = imgres * (imgres > np.max(imgres) * 0.5)   + epsilon\n",
    "        imgres =  (imgres > np.max(imgres) * 0.5)   \n",
    "        \n",
    "        imgres = max_pooling(imgres,pool_size=(4,4))\n",
    "        #imgres = np.log(imgres) - logepsilon\n",
    "        convres.append(imgres)\n",
    "        \n",
    "    return np.array(convres).flatten()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2626b990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T18:24:00.682198Z",
     "start_time": "2023-11-30T18:19:53.262920Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract features from training data \n",
    "save_loc = '''D:/ENTC/PROJECTS/digit-recognition-with-convolution-from-scratch/MNIST Dataset JPG format/'''\n",
    "EXAMPLES_PER_CLASS = 5_000\n",
    "classes = []\n",
    "for integer in range(10):\n",
    "    class_examples_X = []\n",
    "    folder = '''D:/ENTC/PROJECTS/digit-recognition-with-convolution-from-scratch/MNIST Dataset JPG format/MNIST Dataset JPG format/training/''' + str(integer)\n",
    "    filenames = os.listdir(folder)\n",
    "    for example_i in range(min(EXAMPLES_PER_CLASS,len(filenames))):\n",
    "        filepath = folder + '/' + filenames[example_i]\n",
    "        image = cv.imread(filepath,0)\n",
    "        assert image is not None\n",
    "        features_x = get_features(image).tolist()\n",
    "        class_examples_X.append(features_x)\n",
    "    with open('D:/ENTC/PROJECTS/digit-recognition-with-convolution-from-scratch/MNIST Dataset JPG format/training_' + str(integer) + '.json','w') as file:\n",
    "        json.dump(class_examples_X,file)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b283b2c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T19:40:18.871008Z",
     "start_time": "2023-11-30T19:39:26.622818Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract features from testing data \n",
    "save_loc = '''D:/ENTC/PROJECTS/digit-recognition-with-convolution-from-scratch/MNIST Dataset JPG format/'''\n",
    "EXAMPLES_PER_CLASS = 5_000\n",
    "classes = []\n",
    "for integer in range(10):\n",
    "    class_examples_X = []\n",
    "    folder = '''D:/ENTC/PROJECTS/digit-recognition-with-convolution-from-scratch/MNIST Dataset JPG format/MNIST Dataset JPG format/testing/''' + str(integer)\n",
    "    filenames = os.listdir(folder)\n",
    "    for example_i in range(min(EXAMPLES_PER_CLASS,len(filenames))):\n",
    "        filepath = folder + '/' + filenames[example_i]\n",
    "        image = cv.imread(filepath,0)\n",
    "        assert image is not None\n",
    "        features_x = get_features(image).tolist()\n",
    "        class_examples_X.append(features_x)\n",
    "    with open('D:/ENTC/PROJECTS/digit-recognition-with-convolution-from-scratch/MNIST Dataset JPG format/testing_' + str(integer) + '.json','w') as file:\n",
    "        json.dump(class_examples_X,file)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9232267c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T18:48:05.046600Z",
     "start_time": "2023-11-30T18:48:01.958456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retreive saved data for training arrays\n",
    "X_train = None\n",
    "Y_train = None\n",
    "for integer in range(10):\n",
    "    with open('''D:/ENTC/PROJECTS/digit-recognition-with-convolution-from-scratch/MNIST Dataset JPG format/training_''' + str(integer) + '.json') as file:\n",
    "        examples = json.load(file)\n",
    "        examples = np.array(examples).T\n",
    "\n",
    "        if  X_train is None or Y_train is None:\n",
    "            X_train = examples\n",
    "            Y_train = n_ones_in_r_th_row_in_mXn_zero_matrix(10,examples.shape[1],integer)\n",
    "\n",
    "        else:\n",
    "            X_train = np.concatenate((X_train,\n",
    "                                      examples),\n",
    "                                     axis=1)\n",
    "            Y_train = np.concatenate((Y_train,\n",
    "                                      n_ones_in_r_th_row_in_mXn_zero_matrix(10,examples.shape[1],integer)),\n",
    "                                     axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0147d53f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T19:41:50.292594Z",
     "start_time": "2023-11-30T19:41:48.339801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retreive saved data for testing arrays\n",
    "X_test = None\n",
    "Y_test = None\n",
    "for integer in range(10):\n",
    "    with open('''D:/ENTC/PROJECTS/digit-recognition-with-convolution-from-scratch/MNIST Dataset JPG format/testing_''' + str(integer) + '.json') as file:\n",
    "        examples = json.load(file)\n",
    "        examples = np.array(examples).T\n",
    "\n",
    "        if  X_test is None or Y_test is None:\n",
    "            X_test = examples\n",
    "            Y_test = n_ones_in_r_th_row_in_mXn_zero_matrix(10,examples.shape[1],integer)\n",
    "\n",
    "        else:\n",
    "            X_test = np.concatenate((X_test,\n",
    "                                      examples),\n",
    "                                     axis=1)\n",
    "            Y_test = np.concatenate((Y_test,\n",
    "                                      n_ones_in_r_th_row_in_mXn_zero_matrix(10,examples.shape[1],integer)),\n",
    "                                     axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dfe32c6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:00:23.203639Z",
     "start_time": "2023-12-01T05:00:23.180396Z"
    }
   },
   "outputs": [],
   "source": [
    "n_input = X_train.shape[0]  \n",
    "n_output = Y_train.shape[0]\n",
    "\n",
    "my_digit_classifier = DNN(layers=[\n",
    "\n",
    "    {\n",
    "        \"type\":\"input\",\n",
    "        \"units\":n_input\n",
    "    },\n",
    "\n",
    "   \n",
    "    {\n",
    "        \"type\":\"hidden\",\n",
    "        \"units\":10,\n",
    "        \"activation_function\":\"relu\",\n",
    "        \"regularization_strength\":0.0,\n",
    "        \"dropout_keep_prob\":1.0,\n",
    "        \"name\":\"fhl\"\n",
    "    },\n",
    "    \n",
    "    \n",
    "#     {\n",
    "#         \"type\":\"hidden\",\n",
    "#         \"units\":n_input//4,\n",
    "#         \"activation_function\":\"tanh\",\n",
    "#         \"regularization_strength\":1e2,\n",
    "#         \"dropout_keep_prob\":1.0,\n",
    "#         \"name\":\"shl\"\n",
    "#     },\n",
    "    \n",
    "   \n",
    "    {\n",
    "        \"type\":\"output\",\n",
    "        \"units\":n_output,\n",
    "        \"activation_function\":\"sigmoid\",\n",
    "        \"regularization_strength\":0.0,\n",
    "        \"dropout_keep_prob\":1.0\n",
    "    }\n",
    "\n",
    "])\n",
    "\n",
    "%matplotlib nbagg\n",
    "my_digit_classifier.build(weight_init_noise_amplitude=0.01)\n",
    "# my_digit_classifier.show_image(scale=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b5c122cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T05:00:33.364168Z",
     "start_time": "2023-12-01T05:00:25.869935Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__gen_progress_msg_pieces() takes 9 positional arguments but 10 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9576/308595333.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m my_digit_classifier.batch_fit(X_train,Y_train,cost_function=['binary_cross_entropy','least_square'][1],\n\u001b[0m\u001b[0;32m      2\u001b[0m                               \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                              \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_accuracy_calc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                              \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_accuracy_calc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                              \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9576/1756614320.py\u001b[0m in \u001b[0;36mbatch_fit\u001b[1;34m(self, X, Y, cost_function, n_iters, learning_rate, ADAM, beta_1, beta_2, adam_epsilon, test_accuracy, train_accuracy, X_test, Y_test, inline)\u001b[0m\n\u001b[0;32m    265\u001b[0m                         (time_remaining,\n\u001b[0;32m    266\u001b[0m                         \u001b[0mtrain_accuracy_phrase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m                         \u001b[0mtest_accuracy_phrase\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__gen_progress_msg_pieces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m                                                                        X_train,Y_train,test_accuracy,X_test,Y_test)\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __gen_progress_msg_pieces() takes 9 positional arguments but 10 were given"
     ]
    }
   ],
   "source": [
    "my_digit_classifier.batch_fit(X_train,Y_train,cost_function=['binary_cross_entropy','least_square'][1],\n",
    "                              learning_rate=1e-2,n_iters=20,\n",
    "                             test_accuracy=test_accuracy_calc,\n",
    "                             train_accuracy=train_accuracy_calc,\n",
    "                             X_test=X_test,\n",
    "                             Y_test=Y_test)\n",
    "my_digit_classifier.show_cost_history(log=0,grid=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7ee66418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T20:08:52.205757Z",
     "start_time": "2023-11-30T20:08:52.198197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m Reset the parameters according to history state : 0.9100646840889619 \u001b[30m\n",
      "Last training was ignored and returned to the previous state: 0.9100646840889619_params.json\n"
     ]
    }
   ],
   "source": [
    "# my_digit_classifier.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ba33363e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T20:49:17.230532Z",
     "start_time": "2023-11-30T20:49:16.958267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.1\n",
      "test accuracy 0.1135\n"
     ]
    }
   ],
   "source": [
    "#predict and see the accuracy\n",
    "def train_accuracy_calc(model,X_train,Y_train):\n",
    "    Y_pred_logits = model.predict(X_train)\n",
    "    accuracy = np.sum( yres:=Y_train.argmax(axis=0)== Y_pred_logits.argmax(axis=0) )/( yres.shape[0])\n",
    "    return accuracy\n",
    "    \n",
    "def test_accuracy_calc(model,X_test,Y_test):\n",
    "    Y_pred_logits = model.predict(X_test)\n",
    "    accuracy = np.sum( yres:=Y_test.argmax(axis=0)== Y_pred_logits.argmax(axis=0) )/( yres.shape[0])\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"train accuracy\", train_accuracy_calc(my_digit_classifier,X_train,Y_train))\n",
    "print(\"test accuracy\", test_accuracy_calc(my_digit_classifier,X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20ddfd1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T19:02:57.300712Z",
     "start_time": "2023-11-30T19:02:57.284007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m Reset the parameters according to history state : 0.9450524552723802 \u001b[30m\n"
     ]
    }
   ],
   "source": [
    "my_digit_classifier.load_history_state('0.9450524552723802_params.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdca9632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T19:00:27.041106Z",
     "start_time": "2023-11-30T19:00:27.035290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m Reset the parameters according to history state : 0.9450524552723802 \u001b[30m\n",
      "Last training was ignored and returned to the previous state: 0.9450524552723802_params.json\n"
     ]
    }
   ],
   "source": [
    "my_digit_classifier.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36894fbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T19:03:45.722537Z",
     "start_time": "2023-11-30T19:03:45.711886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], [[-1, -2, -1], [0, 0, 0], [1, 2, 1]], [[-1, -1, 1], [-1, -1, -1], [1, -1, 1]], [[-1, -1, 1], [-1, -1, -1], [1, 1, 1]], [[-1, -1, -1], [-1, -1, 1], [-1, 1, -1]]]'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps([kernel.tolist() for kernel in kernels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d81d8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T16:50:56.367018Z",
     "start_time": "2023-11-30T16:50:56.359930Z"
    }
   },
   "outputs": [],
   "source": [
    "start = 5000*9\n",
    "# Y_pred.astype(np.int16)[:,start: start + 10]\n",
    "\n",
    "np.set_string_function(None, repr=False)\n",
    "\n",
    "\n",
    "\n",
    "print(Y_pred_logits[:,start: start + 10].argmax(axis=0))"
   ]
  }
 ],
 "metadata": {
  "author": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
