{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a968a48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T09:58:11.666879Z",
     "start_time": "2023-08-21T09:58:11.304173Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import HTML,Latex\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c0a3fce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T11:33:08.901417Z",
     "start_time": "2023-08-21T11:33:08.881393Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Using the 07--3.1 n-h-1 NN as the starting point and editing\n",
    "\n",
    "Target:-\n",
    "    1. DNN with flexible input size, n_hidden_units in each layer and output units and activation functions\n",
    "    2. Vectorized implementation\n",
    "    3. Include the regularization and drop out as well\n",
    "\n",
    "'''\n",
    "class FAwD: \n",
    "    def __init__(self,\n",
    "                 n_input_neurons=1,\n",
    "                 n_hidden_neurons=12,\n",
    "                 intermidiate_activation_function='sigmoid',\n",
    "                 learning_rate=1e-2,\n",
    "                 n_iters=1_000,\n",
    "                regularization_strength=0,\n",
    "                dropout_keep_prob=1):\n",
    "        self.n_input_neurons = n_input_neurons\n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        \n",
    "        assert intermidiate_activation_function in  ['sigmoid','relu']\n",
    "        self.activation_function = {'sigmoid':sigmoid,'relu':relu}[intermidiate_activation_function]\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.regularization_strength = regularization_strength\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        assert X.shape[0] == self.n_input_neurons\n",
    "        assert y.shape[0] == 1\n",
    "        assert y.shape[1] == X.shape[1], \"Number of examples : should match in inputs and targets\"\n",
    "\n",
    "        self.cost_history = []\n",
    "\n",
    "        # define hyper parameters.\n",
    "        m = y.shape[1] # number of examples given\n",
    "        n = self.n_input_neurons # input feature count\n",
    "        h = self.n_hidden_neurons # hidden neuron count\n",
    "\n",
    "        #initialize weights and biases.\n",
    "        w = np.random.randn(h,1)\n",
    "        b = np.random.randn(1,1)\n",
    "        V = np.random.randn(h,n)\n",
    "        c = np.random.randn(h,1)\n",
    "\n",
    "        #fine tune parameters.\n",
    "        start_time = time.time()\n",
    "        for i in range(self.n_iters):\n",
    "                \n",
    "                # progress message : every 100 iteration except for the first one\n",
    "                if i and not i%100:\n",
    "                    time_now = time.time()\n",
    "                    time_remaining = get_nice_time_dura_str ((self.n_iters - i) * (time_now - start_time) / i)\n",
    "                    print(f\"iteration : {i} ---> ETA : {time_remaining} \",end=\"\\r\")\n",
    "                    \n",
    "\n",
    "                #forward pass\n",
    "                H = self.activation_function(V@X + c)\n",
    "                # drop out\n",
    "                should_drop = np.random.rand(H.shape[0],H.shape[1]) < self.dropout_keep_prob\n",
    "                H *= should_drop\n",
    "                H /= self.dropout_keep_prob\n",
    "                \n",
    "                y_pred = w.T@H + b\n",
    "\n",
    "                #calculate the cost for the current parameters.\n",
    "                cost = 1/(2*m) * np.sum((y_pred - y)**2)\n",
    "                self.cost_history.append(cost)\n",
    "\n",
    "                #backpropagation\n",
    "                diff = (y_pred - y)\n",
    "                \n",
    "                if self.activation_function == sigmoid:\n",
    "                    high_level_derivative = H * (1 - H)\n",
    "                elif self.activation_function == relu:\n",
    "                    high_level_derivative = np.where(H>=0,1.,0.)\n",
    " \n",
    "\n",
    "\n",
    "                dEdb  = diff.sum() / m\n",
    "                dEdw = H@diff.T / m\n",
    "                dEdc = w * (high_level_derivative@diff.T) / m\n",
    "                dEdV = w *  ( (diff * high_level_derivative)@X.T ) / m\n",
    "\n",
    "                # update rule : gradient decent.\n",
    "                # consider regularization too \n",
    "                b *= (1 - self.regularization_strength * self.learning_rate / m)\n",
    "                b -= self.learning_rate * dEdb\n",
    "                w *= (1 - self.regularization_strength * self.learning_rate / m)\n",
    "                w -= self.learning_rate * dEdw\n",
    "                V *= (1 - self.regularization_strength * self.learning_rate / m)\n",
    "                V -= self.learning_rate * dEdV\n",
    "                c *= (1 - self.regularization_strength * self.learning_rate / m)\n",
    "                c -= self.learning_rate * dEdc\n",
    "\n",
    "\n",
    "\n",
    "        #save tuned parameters\n",
    "        self.b = b\n",
    "        self.w = w\n",
    "        self.V = V\n",
    "        self.c = c\n",
    "        \n",
    "        time_now = time.time()\n",
    "        total_time = get_nice_time_dura_str(time_now - start_time)\n",
    "        print(f\"Training ended : n_iters: {self.n_iters} with learning_rate : {self.learning_rate}. Time taken : {total_time}\")\n",
    "        \n",
    "    def show_cost_history(self,log=False):\n",
    "        #plot the cost history\n",
    "        \n",
    "        history_vals = self.cost_history\n",
    "        title = \"Cost vs. iteration\"\n",
    "        if log:\n",
    "            history_vals = np.log(self.cost_history)\n",
    "            title = \"LOG(Cost) vs. iteration\"\n",
    "            \n",
    "        \n",
    "        fig,ax = plt.subplots(1)\n",
    "        ax.plot(history_vals)\n",
    "        ax.set_title(title)\n",
    "        ax.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_final_cost(self):\n",
    "        return self.cost_history[-1]\n",
    "        \n",
    "    def predict(self,X):\n",
    "        \n",
    "        H = self.activation_function(self.V@X + self.c)\n",
    "        y_pred = (self.w).T@H + self.b\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# define activation functions globally\n",
    "def sigmoid(t):\n",
    "        return 1/ ( 1 + np.exp(-t) )\n",
    "      \n",
    "def relu(t): # from chat gpt : this is safe for any dimension array t\n",
    "    return np.maximum(0,t)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# helper functions\n",
    "def get_nice_time_dura_str(time_in_secs):\n",
    "    time_in_secs = round(time_in_secs,2)\n",
    "    if time_in_secs >= 60:\n",
    "        n_mins = int(time_in_secs//60)\n",
    "        n_secs = round(time_in_secs%60,2)\n",
    "        return f\"{n_mins} min {n_secs} secs\"\n",
    "    return f\"{time_in_secs} secs\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "917cb0c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T12:01:34.387781Z",
     "start_time": "2023-08-21T12:01:34.359778Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self,layers):\n",
    "        # layers :  list of dicts containing later information.\n",
    "        assert type(layers) in  (list,tuple)\n",
    "        assert len(layers) >= 2, \"At least an input and an output layer should be there\"\n",
    "        \n",
    "        # validate each layer\n",
    "        for i,layer in enumerate(layers):\n",
    "            if i==0:\n",
    "                assert layer[\"type\"] == \"input\", \"first layer should be input\"\n",
    "            elif i==len(layers)-1:\n",
    "                assert layer[\"type\"] == \"output\", \"last layer should be output\"\n",
    "            else:\n",
    "                assert layer[\"type\"] == \"hidden\", \"middle layers should be hidden\"\n",
    "                assert layer[\"activation_function\"] in (\"relu\",\"sigmoid\",\"linear\")\n",
    "                \n",
    "            assert type(layer[\"units\"]) == int\n",
    "        #done: validation\n",
    "        \n",
    "        #save these info\n",
    "        self.layers = layers\n",
    "        \n",
    "    \n",
    "    def build(self,show=0):\n",
    "        # the parameters will be initialized with required dimentions\n",
    "        \n",
    "        self.W = [None] # this array will keep weight matrix  per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.B = [None] # this array will keep bias vector  per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.activation_def = [None] # the pointer to respecting activation function's definition per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        self.activation_derivative_def = [None] # the pointer to respecting activation function's derivative's  \n",
    "                                                                #definition per each layer (l=1,2,..,L) ... first elem as a placeholde to indeces to work fine\n",
    "        \n",
    "        self.A = [] # keep all intermidiate activation matrix per each layer (l=1,2,..,L)\n",
    "        self.Z = [] # keep all intermidiate pre-activation matrix per each layer (l=1,2,..,L)\n",
    "        \n",
    "        #initializing\n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            # anything for l=0,1,2,..,L ? \n",
    "            # NO\n",
    "             \n",
    "            \n",
    "            if i: #only start from layer 1,2,..,L\n",
    "                weight_matrix = np.random.randn(self.layers[i][\"units\"],self.layers[i-1][\"units\"])\n",
    "                bias_vector = np.zeros((self.layers[i][\"units\"],1))\n",
    "                self.W.append(weight_matrix)\n",
    "                self.B.append(bias_vector)\n",
    "                   \n",
    "               \n",
    "                self.activation_def.append(ACTIVATION_FUNC[self.layers[i][\"activation_function\"]])\n",
    "                self.activation_derivative_def.append(ACTIVATION_FUNC_DERI[self.layers[i][\"activation_function\"]])\n",
    "           \n",
    "            \n",
    "        if show: \n",
    "            print(self.params) \n",
    "            \n",
    "    def batch_fit(self,X,Y,cost_function='least_square',n_iters=1_000,learning_rate=1e-3):\n",
    "        assert cost_function in ('least_square','binary_cross_entropy')\n",
    "        \n",
    "        ##### FWD PASS #####\n",
    "        # r3ki3g : assumes X is pre-scaled / normalized\n",
    "        self.A[0] = X\n",
    "        self.Z[0] = None # not useful but need to store something to indices to work out properly\n",
    "        \n",
    "        # these are some place holders to keep the arrays in required size\n",
    "        self.dZ = [None for i in range(len(self.layers))]\n",
    "        self.dA = [None for i in range(len(self.layers))]\n",
    "        self.dW = [None for i in range(len(self.layers))]\n",
    "        self.dB = [None for i in range(len(self.layers))]\n",
    "        \n",
    "        cost_history = []\n",
    "        \n",
    "        for _ in range(n_iters):\n",
    "                \n",
    "                #current cost calc and storing\n",
    "                cost = \n",
    "            \n",
    "            \n",
    "                # loop though each layer and calculate the pre-activations(Z) and activations(A)\n",
    "                for i in range(1,len(self.layers)):\n",
    "                    self.Z[i] = self.W[i]@self.A[i-1]+self.B[i]\n",
    "                    self.A[i] = self.activation_def[i](self.Z[i])\n",
    "\n",
    "                # now all the activations in the NN are calculated and stored\n",
    "\n",
    "                ##### BACK PROP #####\n",
    "                # The order l = L,L-1,L-2,...,3,2,1 (and no 0)\n",
    "                for i in range(len(self.layers) -1 , 0 , -1): # i=0 is excluded  # note :- L is at  len(self.layers) -1 index\n",
    "\n",
    "                    # dZ[L] depends on the choice of cost function\n",
    "                    if i == len(self.layers) -1:\n",
    "                                if cost_function == 'binary_cross_entropy':\n",
    "                                    self.dZ[i] = self.A[i] - Y\n",
    "                                else:\n",
    "                                    raise Exception(\"not implemented yet\")\n",
    "\n",
    "\n",
    "\n",
    "                    else: # not the last layer\n",
    "                        self.dZ[i] =  (self.W[i+1].T @ self.dZ[i+1]) * self.activation_derivative_def[i](self.Z[i])\n",
    "\n",
    "\n",
    "                    # calculate gradients\n",
    "                    self.dW[i] = self.dZ[i]@self.A[i-1] / m\n",
    "                    self.dB[i] = np.sum(self.dZ[i],axis=1,keepdims=1) / m\n",
    "\n",
    "                    # gradient decent\n",
    "                    self.W[i] -= learning_rate * self.dW[i]\n",
    "                    self.B[i] -= learning_rate * self.dB[i]\n",
    "\n",
    "                \n",
    "                \n",
    "            \n",
    "        \n",
    "                      \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "###### HELPER FUNCTIONS FOR DNN CLASS ###########    \n",
    "    \n",
    "# define activation functions globally\n",
    "def sigmoid(t):\n",
    "        return 1/ ( 1 + np.exp(-t) )\n",
    "      \n",
    "def relu(t): # from chat gpt : this is safe for any dimension array t\n",
    "    return np.maximum(0,t)\n",
    "\n",
    "def relu_deri(t):\n",
    "    return np.where(t>=0,1.,0.)\n",
    "\n",
    "def sigmoid_deri(t):\n",
    "    return (1-t) * t\n",
    "\n",
    "def linear(t):\n",
    "    return t\n",
    "\n",
    "def linear_deri(t):\n",
    "    return 1\n",
    "                \n",
    "ACTIVATION_FUNC = {\"relu\":relu,\"sigmoid\":sigmoid,\"linear\":linear}\n",
    "ACTIVATION_FUNC_DERI = {\"relu\":relu_deri,\"sigmoid\":sigmoid_deri,\"linear\":linear_deri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "458849c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T12:02:02.649580Z",
     "start_time": "2023-08-21T12:02:02.631462Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_dnn = DNN(layers=[\n",
    "    \n",
    "    {\n",
    "        \"type\":\"input\",\n",
    "        \"units\":1\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"type\":\"hidden\",\n",
    "        \"units\":16,\n",
    "        \"activation_function\":\"relu\",\n",
    "        \"regularization_strength\":1e-3,\n",
    "        \"dropout_keep_prob\":1.0\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"type\":\"hidden\",\n",
    "        \"units\":16,\n",
    "        \"activation_function\":\"relu\",\n",
    "        \"regularization_strength\":1e-3,\n",
    "        \"dropout_keep_prob\":1.0\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"type\":\"output\",\n",
    "        \"units\":1,\n",
    "        \"activation_function\":\"sigmoid\",\n",
    "        \"regularization_strength\":1e-3,\n",
    "        \"dropout_keep_prob\":1.0\n",
    "    }\n",
    "    \n",
    "])\n",
    "\n",
    "my_dnn.build()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e910035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T11:47:38.326533Z",
     "start_time": "2023-08-21T11:47:38.310530Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11092/469186585.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "#testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
